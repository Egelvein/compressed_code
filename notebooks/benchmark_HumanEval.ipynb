{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "HwLIWtL0kulU",
   "metadata": {
    "id": "HwLIWtL0kulU"
   },
   "outputs": [],
   "source": [
    "# !pip install jsonlines\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install protobuf\n",
    "# !git clone https://github.com/openai/human-eval\n",
    "# !pip install -e human-eval\n",
    "\n",
    "### RESTART THE KERNEL AFTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b697d999-166a-4d69-b86e-f338031b14d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab456420-263e-4e9c-8ea0-4eaef72a2d6b",
   "metadata": {
    "id": "ab456420-263e-4e9c-8ea0-4eaef72a2d6b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import jsonlines\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from human_eval.data import write_jsonl, read_problems # Install HumanEval first!\n",
    "from human_eval.execution import check_correctness # Install HumanEval first!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57d1a91-8e80-408c-8c5b-96f8bd868128",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Functions for HumanEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "340bb943-44ad-4939-ac20-23cbd6c5d918",
   "metadata": {
    "id": "340bb943-44ad-4939-ac20-23cbd6c5d918"
   },
   "outputs": [],
   "source": [
    "def generate_code_with_transformers(model, tokenizer, prompt: str, num_samples: int = 1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Code generation\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            temperature=0.6,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=num_samples,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    completions = []\n",
    "    for i in range(num_samples):\n",
    "        completion = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        # Keep only code\n",
    "        completion = completion[len(prompt):]\n",
    "        completions.append(completion)\n",
    "\n",
    "    return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "116f4b61-d805-43a8-a3fc-c350df2d63ce",
   "metadata": {
    "id": "116f4b61-d805-43a8-a3fc-c350df2d63ce"
   },
   "outputs": [],
   "source": [
    "def evaluate_hf_model_on_humaneval(model, tokenizer, problems, num_samples_per_task: int = 1):\n",
    "    \"\"\"\n",
    "    Evaluate model on HumanEval.\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for problem_id, problem in problems.items():\n",
    "        \n",
    "        problem_num = problem_id.split(\"/\")[1]\n",
    "        if int(problem_num)%10 == 0:\n",
    "            print(f\"Processing task {problem_id}...\")\n",
    "            \n",
    "        prompt = problem[\"prompt\"]\n",
    "        completions = generate_code_with_transformers(model, tokenizer, prompt, num_samples=num_samples_per_task)\n",
    "\n",
    "        for completion in completions:\n",
    "            result = {\n",
    "                \"task_id\": problem_id,\n",
    "                \"completion\": completion,\n",
    "            }\n",
    "            results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b270508-cea9-42d0-99aa-33a829a42c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_models = {\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q2_K\": \"models/quant_models/Q2_K/Qwen2.5-Coder-7B-Instruct.Q2_K.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q3_K_L\": \"models/quant_models/Q3_K_L/Qwen2.5-Coder-7B-Instruct.Q3_K_L.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q3_K_M\": \"models/quant_models/Q3_K_M/Qwen2.5-Coder-7B-Instruct.Q3_K_M.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q3_K_S\": \"models/quant_models/Q3_K_S/Qwen2.5-Coder-7B-Instruct.Q3_K_S.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q4_0\": \"models/quant_models/Q4_0/Qwen2.5-Coder-7B-Instruct.Q4_0.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q4_1\": \"models/quant_models/Q4_1/Qwen2.5-Coder-7B-Instruct.Q4_1.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q4_K_M\": \"models/quant_models/Q4_K_M/Qwen2.5-Coder-7B-Instruct.Q4_K_M.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q4_K_S\": \"models/quant_models/Q4_K_S/Qwen2.5-Coder-7B-Instruct.Q4_K_S.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q5_0\": \"models/quant_models/Q5_0/Qwen2.5-Coder-7B-Instruct.Q5_0.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q5_1\": \"models/quant_models/Q5_1/Qwen2.5-Coder-7B-Instruct.Q5_1.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q5_K_M\": \"models/quant_models/Q5_K_M/Qwen2.5-Coder-7B-Instruct.Q5_K_M.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q5_K_S\": \"models/quant_models/Q5_K_S/Qwen2.5-Coder-7B-Instruct.Q5_K_S.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q6_K\": \"models/quant_models/Q6_K/Qwen2.5-Coder-7B-Instruct.Q6_K.gguf\",\n",
    "    \"Qwen2.5-Coder-7B-Instruct.Q8_0\": \"models/quant_models/Q8_0/Qwen2.5-Coder-7B-Instruct.Q8_0.gguf\",\n",
    "}\n",
    "\n",
    "base_models = {\n",
    "    \"Qwen2.5-1.5B\": \"models/base_models/Qwen2.5-1.5B\", +\n",
    "    \"Qwen2.5-1.5B-Instruct\": \"models/base_models/Qwen2.5-1.5B-Inst\", # +\n",
    "    \"Qwen2.5-Coder-1.5B\": \"models/base_models/Qwen2.5-Coder-1.5B\", # +\n",
    "    \"Qwen2.5-Coder-1.5B-Instruct\": \"models/base_models/Qwen2.5-Coder-1.5B-Inst\", # +\n",
    "    \"Qwen2.5-7B\": \"models/base_models/Qwen2.5-7B\", # +\n",
    "    \"Qwen2.5-7B-Instruct\": \"models/base_models/Qwen2.5-7B-Inst\", # +\n",
    "    \"Qwen2.5-Coder-7B\": \"models/base_models/Qwen2.5-Coder-7B\", # +\n",
    "    \"Qwen2.5-Coder-7B-Instruct\": \"models/base_models/Qwen2.5-Coder-7B-Inst\", # +\n",
    "    \"Qwen2.5-14B\": \"models/base_models/Qwen2.5-14B\",\n",
    "    \"Qwen2.5-14B-Instruct\": \"models/base_models/Qwen2.5-14B-Inst\",\n",
    "    \"Qwen2.5-Coder-14B\": \"models/base_models/Qwen2.5-Coder-14B\",\n",
    "    \"Qwen2.5-Coder-14B-Instruct\": \"models/base_models/Qwen2.5-Coder-14B-Inst\",\n",
    "}\n",
    "\n",
    "distill_models = {\n",
    "    \"DeepSeek-R1-Distill-Llama-70B\" : \"models/distill_models/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-32B\" : \"models/distill_models/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    \"DeepSeek-R1-Distill-Qwen-14B\" : \"models/distill_models/DeepSeek-R1-Distill-Qwen-14B\", # +\n",
    "    \"DeepSeek-R1-Distill-Llama-8B\": \"models/distill_models/DeepSeek-R1-Distill-Llama-8B\", # +\n",
    "    \"DeepSeek-R1-Distill-Qwen-7B\" : \"models/distill_models/DeepSeek-R1-Distill-Qwen-7B\", # +\n",
    "    \"DeepSeek-R1-Distill-Qwen-1.5B\" : \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\", # +\n",
    "}\n",
    "\n",
    "qwen3_models = {\n",
    "    'Qwen3-0.6B-Base': 'models/qwen3_models/Qwen3-0.6B-Base', # +\n",
    "    'Qwen3-0.6B-FP8': 'models/qwen3_models/Qwen3-0.6B-FP8',\n",
    "    'Qwen3-0.6B': 'models/qwen3_models/Qwen3-0.6B', +\n",
    "    'Qwen3-1.7B-Base': 'models/qwen3_models/Qwen3-1.7B-Base', # +\n",
    "    'Qwen3-1.7B-FP8': 'models/qwen3_models/Qwen3-1.7B-FP8',\n",
    "    'Qwen3-1.7B': 'models/qwen3_models/Qwen3-1.7B', # +\n",
    "    'Qwen3-4B-Base': 'models/qwen3_models/Qwen3-4B-Base', # +\n",
    "    'Qwen3-4B-FP8': 'models/qwen3_models/Qwen3-4B-FP8',\n",
    "    'Qwen3-4B': 'models/qwen3_models/Qwen3-4B', # +\n",
    "    'Qwen3-8B-Base': 'models/qwen3_models/Qwen3-8B-Base', # +\n",
    "    'Qwen3-8B-FP8': 'models/qwen3_models/Qwen3-8B-FP8',\n",
    "    'Qwen3-8B': 'models/qwen3_models/Qwen3-8B',\n",
    "    'Qwen3-14B-Base': 'models/qwen3_models/Qwen3-14B-Base',\n",
    "    'Qwen3-14B-FP8': 'models/qwen3_models/Qwen3-14B-FP8',\n",
    "    'Qwen3-14B': 'models/qwen3_models/Qwen3-14B',\n",
    "    'Qwen3-30B-A3B-Base': 'models/qwen3_models/Qwen3-30B-A3B-Base',\n",
    "    'Qwen3-30B-A3B-FP8': 'models/qwen3_models/Qwen3-30B-A3B-FP8',\n",
    "    'Qwen3-30B-A3B': 'models/qwen3_models/Qwen3-30B-A3B',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f003ca6-3d80-422c-a5ef-564f40f23b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problems = read_problems()\n",
    "len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb6b411b-70de-4662-b975-ff052480912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(problems['HumanEval/0']['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ae527-bbdd-4110-ab33-78709310684d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae1675f8-3947-4e6d-81d6-784428b3e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.bfloat16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84792e3d-c3fe-43f0-b661-f238d0ef8808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = evaluate_hf_model_on_humaneval(model.bfloat16(), tokenizer, problems, num_samples_per_task=1) # Pass@1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a73b89-ba95-40e4-826f-06bf9607af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTPUT_DIR = \"HE_results\"\n",
    "\n",
    "for model_name, model_path in base_models.items():\n",
    "    try:\n",
    "        del model\n",
    "        del results\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(model_name)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "    results = evaluate_hf_model_on_humaneval(model, tokenizer, problems, num_samples_per_task=1) # Pass@1\n",
    "    write_jsonl(f\"{OUTPUT_DIR}/{model_name.replace('/', '_')}_results.jsonl\", results)\n",
    "    print(f\"Evaluation complete. Results saved to {OUTPUT_DIR}/{model_name.replace('/', '_')}_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea1fa00-41e4-414a-9541-b2e1034efbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTPUT_DIR = \"HE_results\"\n",
    "\n",
    "for model_name, model_path in qwen3_models.items():\n",
    "    try:\n",
    "        del model\n",
    "        del results\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(model_name)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "    \n",
    "    results = evaluate_hf_model_on_humaneval(model, tokenizer, problems, num_samples_per_task=1) # Pass@1\n",
    "    write_jsonl(f\"{OUTPUT_DIR}/{model_name.replace('/', '_')}_results.jsonl\", results)\n",
    "    print(f\"Evaluation complete. Results saved to {OUTPUT_DIR}/{model_name.replace('/', '_')}_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c4c219-526b-4ac6-82f2-5266769f3bad",
   "metadata": {
    "id": "18c4c219-526b-4ac6-82f2-5266769f3bad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    for i in range(len(numbers)):\n",
      "        for j in range(i + 1, len(numbers)):\n",
      "            if abs(numbers[i] - numbers[j]) < threshold:\n",
      "                return True\n",
      "    return False\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    from doctest import testmod\n",
      "\n",
      "    testmod()\n"
     ]
    }
   ],
   "source": [
    "print(results[0]['completion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb39685-787e-464f-bee6-b335dd7cded6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Distill models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354ae226-a9d8-45f1-b516-448b7c8cfcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "OUTPUT_DIR = \"HE_results\"\n",
    "\n",
    "for model_name, model_path in distill_models.items():\n",
    "    try:\n",
    "        del model\n",
    "        del results\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    print(model_name)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "\n",
    "    results = evaluate_hf_model_on_humaneval(model, tokenizer, problems, num_samples_per_task=1) # Pass@1\n",
    "    write_jsonl(f\"{OUTPUT_DIR}/{model_name.replace('/', '_')}_results.jsonl\", results)\n",
    "    print(f\"Evaluation complete. Results saved to {OUTPUT_DIR}/{model_name.replace('/', '_')}_results.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b726a1-8692-4cff-8121-5e539acfff9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8009d1ca-10f0-49e8-b893-38c3224a3591",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ec6dba-476f-4c0d-b650-23358cb32069",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Base Qwen2.5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2befc16d-22af-48a1-ae6b-f4c494529804",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2befc16d-22af-48a1-ae6b-f4c494529804",
    "outputId": "ec695faf-a44f-4db3-fc2f-1a335a02d406"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 9006.66it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:01<00:00, 87.91it/s]\n",
      "Writing results to HE_results/Qwen2.5-Coder-1.5B-Instruct_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 9230.13it/s]\n",
      "{'pass@1': 0.13414634146341464}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-Coder-1.5B-Instruct_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ce1b2a7-a6b2-4e87-9175-41820fda1bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 10980.55it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 48.55it/s]\n",
      "Writing results to HE_results/Qwen2.5-Coder-1.5B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 10896.02it/s]\n",
      "{'pass@1': 0.1951219512195122}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-Coder-1.5B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8c551f6-284f-4019-85df-ee9ca9ba4f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 11389.64it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 48.70it/s]\n",
      "Writing results to HE_results/Qwen2.5-1.5B-Instruct_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 6040.22it/s]\n",
      "{'pass@1': 0.21341463414634146}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-1.5B-Instruct_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f3ee315-e9d5-447a-8978-8e1994f851f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 6350.31it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 46.91it/s]\n",
      "Writing results to HE_results/Qwen2.5-1.5B_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 9545.87it/s]\n",
      "{'pass@1': 0.1951219512195122}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-1.5B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60434d1c-e2a4-4f8b-ac07-dacecc2b0463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 11848.52it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:02<00:00, 78.29it/s]\n",
      "Writing results to HE_results/Qwen2.5-7B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 13124.21it/s]\n",
      "{'pass@1': 0.27439024390243905}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-7B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca71bbf-7773-4135-bbba-ceadf217f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 3796.97it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:02<00:00, 62.33it/s]\n",
      "Writing results to HE_results/Qwen2.5-7B-Instruct_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 7753.66it/s]\n",
      "{'pass@1': 0.18292682926829268}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-7B-Instruct_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeacdea8-d224-4b39-aa79-3d243e18f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 4039.69it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:01<00:00, 95.70it/s]\n",
      "Writing results to HE_results/Qwen2.5-Coder-7B_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 5688.09it/s]\n",
      "{'pass@1': 0.2865853658536585}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-Coder-7B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d79978a3-c529-49d2-ab93-8723ce8cb1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 13923.85it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:02<00:00, 65.55it/s]\n",
      "Writing results to HE_results/Qwen2.5-Coder-7B-Instruct_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 5806.88it/s]\n",
      "{'pass@1': 0.24390243902439024}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-Coder-7B-Instruct_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb13f0de-126d-4278-9511-98a6bd95dc40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 10306.50it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 50.53it/s]\n",
      "Writing results to Qwen2.5-Coder-7B-Instruct.Q2_K.gguf_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 14663.21it/s]\n",
      "{'pass@1': 0.11585365853658537}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py Qwen2.5-Coder-7B-Instruct.Q2_K.gguf_results.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e55006-a09a-4a7b-b320-831206bb3ed9",
   "metadata": {},
   "source": [
    "## Distill R1 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a338f507-b738-4fd9-8073-ba57617cab59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 15308.02it/s]\n",
      "Running test suites...\n",
      "100%|████████████████████████████████████████| 164/164 [00:01<00:00, 135.48it/s]\n",
      "Writing results to HE_results/DeepSeek-R1-Distill-Llama-8B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 17516.77it/s]\n",
      "{'pass@1': 0.08536585365853659}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/DeepSeek-R1-Distill-Llama-8B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ae70a8-2878-44b9-8e0d-de5f5bc33ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 9365.60it/s]\n",
      "Running test suites...\n",
      "100%|████████████████████████████████████████| 164/164 [00:01<00:00, 131.11it/s]\n",
      "Writing results to HE_results/DeepSeek-R1-Distill-Qwen-7B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 10571.65it/s]\n",
      "{'pass@1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/DeepSeek-R1-Distill-Qwen-7B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6e63148-45c5-476e-b855-5345339dee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 6475.31it/s]\n",
      "Running test suites...\n",
      "100%|████████████████████████████████████████| 164/164 [00:01<00:00, 130.94it/s]\n",
      "Writing results to HE_results/DeepSeek-R1-Distill-Qwen-1.5B_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 4819.45it/s]\n",
      "{'pass@1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/DeepSeek-R1-Distill-Qwen-1.5B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053aa59a-4d51-4f8f-817d-3f8c260de602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 15423.00it/s]\n",
      "Running test suites...\n",
      "100%|████████████████████████████████████████| 164/164 [00:01<00:00, 144.21it/s]\n",
      "Writing results to HE_results/DeepSeek-R1-Distill-Qwen-14B_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 6462.11it/s]\n",
      "{'pass@1': 0.06097560975609756}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/DeepSeek-R1-Distill-Qwen-14B_results.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344bdb06-f903-4cd5-8302-f829da93ab8c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Base Qwen3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df222170-c5c6-4422-8f95-98fa3c5968c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 16312.12it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:04<00:00, 39.61it/s]\n",
      "Writing results to HE_results/Qwen3-0.6B-Base_results.jsonl_results.jsonl...\n",
      "100%|███████████████████████████████████████| 164/164 [00:00<00:00, 9824.69it/s]\n",
      "{'pass@1': 0.12195121951219512}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-0.6B-Base_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c1caa8a-e794-4366-a0eb-197664d96a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 9675.03it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 47.56it/s]\n",
      "Writing results to HE_results/Qwen3-0.6B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 10282.92it/s]\n",
      "{'pass@1': 0.042682926829268296}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-0.6B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a599abf5-0e22-4164-bb4f-61bb9973d13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 15645.76it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 50.46it/s]\n",
      "Writing results to HE_results/Qwen3-1.7B-Base_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 21745.89it/s]\n",
      "{'pass@1': 0.1951219512195122}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-1.7B-Base_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "742d0b66-328e-4a46-a99c-f660a51ab7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 14485.96it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 52.45it/s]\n",
      "Writing results to HE_results/Qwen3-1.7B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 17173.03it/s]\n",
      "{'pass@1': 0.024390243902439025}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-1.7B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d324eff-28b5-4267-a305-024f7e4a42be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 17335.33it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 42.01it/s]\n",
      "Writing results to HE_results/Qwen3-4B-Base_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 23379.30it/s]\n",
      "{'pass@1': 0.2621951219512195}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-4B-Base_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42dbfd9c-80b7-4da3-8341-3b4992a26c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 12318.07it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:02<00:00, 60.69it/s]\n",
      "Writing results to HE_results/Qwen3-4B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 10896.54it/s]\n",
      "{'pass@1': 0.1951219512195122}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-4B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61775479-3ee7-4a6b-91b8-2072395ffff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 16246.63it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:03<00:00, 48.44it/s]\n",
      "Writing results to HE_results/Qwen3-8B-Base_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 21995.52it/s]\n",
      "{'pass@1': 0.2804878048780488}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-8B-Base_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "921003b6-73e1-48ae-adbf-fadd5d636456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 18149.97it/s]\n",
      "Running test suites...\n",
      "100%|█████████████████████████████████████████| 164/164 [00:02<00:00, 78.22it/s]\n",
      "Writing results to HE_results/Qwen3-8B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 10686.62it/s]\n",
      "{'pass@1': 0.11585365853658537}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen3-8B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ce8aae-e4fb-475f-99b0-a7284e7bbc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading samples...\n",
      "164it [00:00, 24123.79it/s]\n",
      "Running test suites...\n",
      "100%|████████████████████████████████████████| 164/164 [00:01<00:00, 102.65it/s]\n",
      "Writing results to HE_results/Qwen2.5-14B_results.jsonl_results.jsonl...\n",
      "100%|██████████████████████████████████████| 164/164 [00:00<00:00, 34159.30it/s]\n",
      "{'pass@1': 0.0}\n"
     ]
    }
   ],
   "source": [
    "!python human-eval/human_eval/evaluate_functional_correctness.py HE_results/Qwen2.5-14B_results.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d88a9e-39e0-4f65-b213-093bfdeeaf91",
   "metadata": {
    "id": "20d88a9e-39e0-4f65-b213-093bfdeeaf91"
   },
   "outputs": [],
   "source": [
    "#evaluate_hf_model_on_humaneval(model_name, problems, num_samples_per_task=10)  # Pass@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df286c56-f506-44d6-a1d8-63f2e2c5d3c4",
   "metadata": {
    "id": "df286c56-f506-44d6-a1d8-63f2e2c5d3c4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c4d6e-9b96-4424-ae60-eebab3faffc5",
   "metadata": {
    "id": "845c4d6e-9b96-4424-ae60-eebab3faffc5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
