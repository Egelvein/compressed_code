{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10546507,"sourceType":"datasetVersion","datasetId":6525356}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom itertools import islice\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    PreTrainedTokenizerFast,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:04:50.672346Z","iopub.execute_input":"2025-01-27T09:04:50.672625Z","iopub.status.idle":"2025-01-27T09:04:50.685584Z","shell.execute_reply.started":"2025-01-27T09:04:50.672600Z","shell.execute_reply":"2025-01-27T09:04:50.684932Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Upload tools","metadata":{}},{"cell_type":"code","source":"# qwen_coder_model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n# qwen_coder_model = AutoModelForCausalLM.from_pretrained(\n#     qwen_coder_model_name,\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\"\n# )\n\n\n# qwen_model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n# qwen_model = AutoModelForCausalLM.from_pretrained(\n#     qwen_model_name,\n#     torch_dtype=\"auto\",\n#     device_map=\"auto\"\n# )\n\n\n# athene_model_name = \"Nexusflow/Athene-V2-Chat\"\n# athene_model = AutoModelForCausalLM.from_pretrained(athene_model_name)\n\n\n# deepseek_model_name = \"deepseek-ai/DeepSeek-V2-Lite\"\n# deepseek_model = AutoModelForCausalLM.from_pretrained(deepseek_model_name,\n#                                                       trust_remote_code=True)  \n\n\n# deepseek_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n# deepseek_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\n\n# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)  \n\n\n# athene_tokenizer = AutoTokenizer.from_pretrained(\"Nexusflow/Athene-V2-Chat\")\n# athene_model = AutoModelForCausalLM.from_pretrained(\"Nexusflow/Athene-V2-Chat\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:00:45.687788Z","iopub.execute_input":"2025-01-27T09:00:45.688221Z","iopub.status.idle":"2025-01-27T09:02:27.517430Z","shell.execute_reply.started":"2025-01-27T09:00:45.688189Z","shell.execute_reply":"2025-01-27T09:02:27.516806Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b5a787175e4a91a595dd5c534c459d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed20142a506141f595b6d601e0a10e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708dbdfc85394de68e44d8ecb60c6f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac48de397134ded820c733b8d4c96f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73d9faf518e49e8aaafa115b98b4649"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# qwen_tokenizer_path = \"/kaggle/input/tokenizers/qwen_tokenizer.json\"\n# deepseek_tokenizer_path = \"/kaggle/input/tokenizers/deepseek_tokenizer.json\"\n# llama_tokenizer_path = \"/kaggle/input/tokenizers/llama_tokenizer.json\"\n\n# with open(qwen_tokenizer_path, 'r') as file:\n#     qwen_tokenizer = PreTrainedTokenizerFast(tokenizer_file=qwen_tokenizer_path)\n#     qwen = json.load(file)\n#     qwen_vocab = qwen['model']['vocab']\n# with open(deepseek_tokenizer_path, 'r') as file:\n#     deepseek_tokenizer = PreTrainedTokenizerFast(tokenizer_file=deepseek_tokenizer_path)\n#     deepseek = json.load(file)\n#     deepseek_vocab = deepseek['model']['vocab']\n# with open(llama_tokenizer_path, 'r') as file:\n#     llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=llama_tokenizer_path)\n#     llama = json.load(file)\n#     llama_vocab = llama['model']['vocab']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:56:05.001929Z","iopub.execute_input":"2025-01-27T08:56:05.002480Z","iopub.status.idle":"2025-01-27T08:56:05.551250Z","shell.execute_reply.started":"2025-01-27T08:56:05.002453Z","shell.execute_reply":"2025-01-27T08:56:05.550520Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"python_keywords = [\n    \"def\", \"class\", \"import\", \"from\", \"as\", \"if\", \"elif\", \"else\", \"while\", \n    \"for\", \"in\", \"break\", \"continue\", \"return\", \"yield\", \"try\", \"except\", \n    \"finally\", \"with\", \"assert\", \"lambda\", \"global\", \"nonlocal\", \"pass\", \n    \"raise\", \"True\", \"False\", \"None\", \"is\", \"not\"\n]\n\njava_keywords = [\n    \"class\", \"interface\", \"extends\", \"implements\", \"package\", \"import\", \"public\", \n    \"private\", \"protected\", \"static\", \"final\", \"abstract\", \"synchronized\", \n    \"volatile\", \"transient\", \"native\", \"strictfp\", \"void\", \"int\", \"double\", \n    \"float\", \"char\", \"boolean\", \"long\", \"short\", \"byte\", \"if\", \"else\", \"while\",\n    \"for\"\n]\n\n\ncpp_keywords = [\n    \"int\", \"float\", \"double\", \"char\", \"void\", \"bool\", \"short\", \"long\", \"signed\", \n    \"unsigned\", \"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \n    \"do\", \"break\", \"continue\", \"return\", \"goto\", \"class\", \"struct\", \"union\", \n    \"namespace\", \"using\", \"private\", \"protected\", \"public\"\n]\n\ngo_keywords = [\n    \"break\", \"case\", \"chan\", \"const\", \"continue\", \"default\", \"defer\", \"else\", \n    \"fallthrough\", \"for\", \"func\", \"go\", \"goto\", \"if\", \"import\", \"interface\", \n    \"map\", \"package\", \"range\", \"return\", \"select\", \"struct\", \"switch\", \"type\", \n    \"var\", \"bool\", \"byte\", \"complex64\", \"complex128\", \"float32\"\n]\n\nreact_keywords = [\n    \"React\", \"Component\", \"useState\", \"useEffect\", \"useContext\", \"useReducer\", \n    \"useMemo\", \"useCallback\", \"useRef\", \"useLayoutEffect\", \"useImperativeHandle\", \n    \"jsx\", \"props\", \"state\", \"context\", \"provider\", \"consumer\", \"hook\", \n    \"fragment\", \"key\", \"ref\", \"render\", \"virtualDOM\", \"reconciliation\", \n    \"portal\", \"suspense\", \"lazy\", \"errorBoundary\", \"children\", \"strictMode\"\n]\n\njavascript_keywords = [\n    \"var\", \"let\", \"const\", \"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \n    \"while\", \"do\", \"break\", \"continue\", \"return\", \"function\", \"class\", \n    \"extends\", \"constructor\", \"super\", \"import\", \"export\", \"try\",\n    \"catch\", \"finally\", \"throw\", \"this\", \"new\", \"delete\", \"instanceof\", \"typeof\"\n]\n\nrust_keywords = [\n    \"as\", \"break\", \"const\", \"continue\", \"crate\", \"else\", \"enum\", \"extern\", \n    \"false\", \"fn\", \"for\", \"if\", \"impl\", \"in\", \"let\", \"loop\", \"match\", \n    \"mod\", \"move\", \"mut\", \"pub\", \"ref\", \"return\", \"Self\", \"self\", \"static\", \n    \"struct\", \"super\", \"trait\", \"true\"\n]\n\nphp_keywords = [\n    \"abstract\", \"and\", \"array\", \"as\", \"break\", \"callable\", \"case\", \"catch\", \n    \"class\", \"clone\", \"const\", \"continue\", \"declare\", \"default\", \"do\", \"echo\", \n    \"else\", \"elseif\", \"empty\", \"enddeclare\", \"endfor\", \"endforeach\", \"endif\", \n    \"endswitch\", \"endwhile\", \"eval\", \"exit\", \"extends\", \"final\", \"finally\"\n]\n\nall_keywords = [python_keywords,\n                java_keywords,\n                cpp_keywords,\n                go_keywords,\n                react_keywords,\n                javascript_keywords,\n                rust_keywords,\n                php_keywords,]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:04:50.686307Z","iopub.execute_input":"2025-01-27T09:04:50.686596Z","iopub.status.idle":"2025-01-27T09:04:50.701879Z","shell.execute_reply.started":"2025-01-27T09:04:50.686576Z","shell.execute_reply":"2025-01-27T09:04:50.701196Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Warm start","metadata":{}},{"cell_type":"code","source":"def get_token_probabilities(model, tokenizer, text, top_k=None):\n    \"\"\"\n    Get token probabilities from a GPT model for each position in the input text.\n\n    Args:\n        model: GPT model instance\n        tokenizer: GPT tokenizer instance\n        text: Input text to analyze\n        top_k: Optional, number of top probabilities to return for each position\n\n    Returns:\n        List of dictionaries containing token probabilities for each position\n    \"\"\"\n    # Tokenize input text\n    input_ids = tokenizer.encode(text, return_tensors='pt')\n\n    # Get model's raw predictions\n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits\n\n    # Convert logits to probabilities using softmax\n    probs = F.softmax(logits[0], dim=-1)\n\n    # Store results for each position\n    token_probs = []\n\n    # Analyze each position\n    for position in range(len(input_ids[0])):\n        position_probs = probs[position]\n\n        # Get token indices sorted by probability\n        sorted_indices = torch.argsort(position_probs, descending=True)\n\n        # Limit to top_k if specified\n        if top_k:\n            sorted_indices = sorted_indices[:top_k]\n\n        # Create dictionary of token:probability pairs\n        position_dict = {\n            'position': position,\n            'token': tokenizer.decode(input_ids[0][position]),\n            'probabilities': {\n                tokenizer.decode([idx.item()]): position_probs[idx].item()\n                for idx in sorted_indices\n            }\n        }\n\n        token_probs.append(position_dict)\n\n    return token_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:04:50.702578Z","iopub.execute_input":"2025-01-27T09:04:50.702778Z","iopub.status.idle":"2025-01-27T09:04:50.718114Z","shell.execute_reply.started":"2025-01-27T09:04:50.702752Z","shell.execute_reply":"2025-01-27T09:04:50.717059Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Example usage\ndef analyze_text(text, model, tokenizer, top_k=50):\n    \"\"\"\n    Analyze token probabilities for a given text using specified GPT model.\n\n    Args:\n        text: Input text to analyze\n        model_name: Name of the GPT model to use\n        top_k: Number of top probabilities to show for each position\n    \"\"\"\n    # Load model and tokenizer\n    # model = GPT2LMHeadModel.from_pretrained(model_name)\n    # tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\n    # Get token probabilities\n    results = get_token_probabilities(model, tokenizer, text, top_k)\n    \n    # Print results\n    # print(f\"Analysis of text: '{text}'\\n\")\n    # for pos_data in results[:5]:\n    #     print(f\"Position {pos_data['position']}: Token '{pos_data['token']}'\")\n    #     print(\"Top probabilities:\")\n    #     for token, prob in islice(pos_data['probabilities'].items(), 3):\n    #         print(f\"->{token}<-: {prob:.4f}\")\n    #     print()\n        \n    tokens_prob = {}\n    for i in range(len(results)):\n        given_token = results[i]['token']\n        tokens_prob[given_token] = {}\n        \n        for token, prob in islice(results[i]['probabilities'].items(), 3):\n            tokens_prob[given_token][token] = prob\n\n    return tokens_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:04:50.719035Z","iopub.execute_input":"2025-01-27T09:04:50.719549Z","iopub.status.idle":"2025-01-27T09:04:50.735586Z","shell.execute_reply.started":"2025-01-27T09:04:50.719465Z","shell.execute_reply":"2025-01-27T09:04:50.735073Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Check on specific programming words","metadata":{}},{"cell_type":"code","source":"from typing import List\n\ndef warm_start(words: List[str], model, tokenizer):\n    words = ' '.join(words)\n    result = analyze_text(words, model, tokenizer)\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:04:50.738629Z","iopub.execute_input":"2025-01-27T09:04:50.738938Z","iopub.status.idle":"2025-01-27T09:04:50.755835Z","shell.execute_reply.started":"2025-01-27T09:04:50.738911Z","shell.execute_reply":"2025-01-27T09:04:50.754881Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Qwen2.5-coder","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"with open(\"qwen-2-5-coder.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, qwen_coder_model, qwen_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n    \n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:57:20.356420Z","iopub.execute_input":"2025-01-27T08:57:20.356730Z","iopub.status.idle":"2025-01-27T08:57:26.348634Z","shell.execute_reply.started":"2025-01-27T08:57:20.356707Z","shell.execute_reply":"2025-01-27T08:57:26.347694Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Qwen-2.5","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"with open(\"qwen-2-5.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, qwen_model, qwen_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:47:27.967075Z","iopub.execute_input":"2025-01-27T08:47:27.967388Z","iopub.status.idle":"2025-01-27T08:47:34.224575Z","shell.execute_reply.started":"2025-01-27T08:47:27.967365Z","shell.execute_reply":"2025-01-27T08:47:34.223889Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Athene","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"with open(\"athene.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, athene_model, athene_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-22T09:31:30.951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Deepseek r1","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"with open(\"deepseek-r1.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DeepSeek-R1-Distill-Qwen-7B","metadata":{}},{"cell_type":"code","source":"deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\ndeepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n\nwith open(\"deepseek-r1-qwen-7b.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:04:58.936890Z","iopub.execute_input":"2025-01-27T09:04:58.937173Z","execution_failed":"2025-01-27T09:16:54.877Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e06744ee074fe7b868d070d0de308a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255137e1ce9e4a35b236c16ec4b82767"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d4dfe3e32e24ca8bc054ba1f7356b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/28.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf066d4d346d47ffa77489f5fd4d9922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f7528f2cfa41aeb144d497c1b2abd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000002.safetensors:   0%|          | 0.00/8.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c872e12e5164ce98367a0f210a048f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-000002.safetensors:   0%|          | 0.00/6.62G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b98ff6a9a1147cf8192ad5b925df217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854717ebc156446c88e723f0dd4844e8"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"### DeepSeek-R1-Distill-Qwen-1.5B","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\ndeepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\nwith open(\"deepseek-r1-qwen-1.5b.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n    \n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:02:27.572150Z","iopub.execute_input":"2025-01-27T09:02:27.572388Z","iopub.status.idle":"2025-01-27T09:02:45.491814Z","shell.execute_reply.started":"2025-01-27T09:02:27.572367Z","shell.execute_reply":"2025-01-27T09:02:45.491165Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### DeepSeek-R1-Distill-Llama-8B ","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"cell_type":"code","source":"deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\ndeepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n\nwith open(\"deepseek-r1-llama-8b.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Miss words in tokenizers","metadata":{}},{"cell_type":"code","source":"misswords = ['nonlocal',\n             'synchronized',\n             'transient',\n             'strictfp',\n             'fallthrough',\n             'complex64',\n             'complex128',\n             'float32',\n             'useEffect',\n             'useContext',\n             'useReducer',\n             'useMemo',\n             'useCallback',\n             'useRef',\n             'useLayoutEffect',\n             'useImperativeHandle',\n             'virtualDOM',\n             'reconciliation',\n             'suspense',\n             'errorBoundary',\n             'useStrictMode',\n             'instanceof',\n             'enddeclare',\n             'endfor',\n             'endswitch',\n             'endwhile',\n            ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:31:14.131900Z","iopub.execute_input":"2025-01-22T11:31:14.132346Z","iopub.status.idle":"2025-01-22T11:31:14.136957Z","shell.execute_reply.started":"2025-01-22T11:31:14.132309Z","shell.execute_reply":"2025-01-22T11:31:14.135923Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import random\nfrom termcolor import colored\n\ncolors = ['red', 'green', 'yellow', 'blue']\n\ndef tokenize_and_colorize(tokenizer, code):\n    tokens = tokenizer.tokenize(code)\n    colored_code = []\n    last_color = None\n\n    for tok in tokens:\n        new_color = random.choice([color for color in colors if color != last_color])\n        last_color = new_color\n        colored_code.append(colored(tok, new_color))\n\n    return ' '.join(colored_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:46:37.883595Z","iopub.execute_input":"2025-01-22T11:46:37.883912Z","iopub.status.idle":"2025-01-22T11:46:37.888916Z","shell.execute_reply.started":"2025-01-22T11:46:37.883889Z","shell.execute_reply":"2025-01-22T11:46:37.887912Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## Qwen2","metadata":{}},{"cell_type":"code","source":"for missword in misswords:\n    colored_code = tokenize_and_colorize(qwen_tokenizer, missword)\n    print(colored_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:18:58.276089Z","iopub.execute_input":"2025-01-22T12:18:58.276760Z","iopub.status.idle":"2025-01-22T12:18:58.293479Z","shell.execute_reply.started":"2025-01-22T12:18:58.276703Z","shell.execute_reply":"2025-01-22T12:18:58.292723Z"}},"outputs":[{"name":"stdout","text":"non local\ns ynchronized\ntrans ient\nstrict fp\nfall through\ncomplex 6 4\ncomplex 1 2 8\nfloat 3 2\nuse Effect\nuse Context\nuse Reducer\nuse Memo\nuse Callback\nuse Ref\nuse Layout Effect\nuse Im per ative Handle\nvirtual DOM\nre conciliation\ns usp ense\nerror Boundary\nuse Strict Mode\ninstance of\nend declare\nend for\nends witch\nend while\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Deepseek r1","metadata":{}},{"cell_type":"code","source":"for missword in misswords:\n    colored_code = tokenize_and_colorize(deepseek_tokenizer, missword)\n    print(colored_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:46:40.549551Z","iopub.execute_input":"2025-01-22T11:46:40.549892Z","iopub.status.idle":"2025-01-22T11:46:40.564981Z","shell.execute_reply.started":"2025-01-22T11:46:40.549860Z","shell.execute_reply":"2025-01-22T11:46:40.564079Z"}},"outputs":[{"name":"stdout","text":"non local\ns ynchronized\ntrans ient\nstrict fp\nfall through\ncomplex 6 4\ncomplex 1 2 8\nfloat 3 2\nuse Effect\nuse Context\nuse Reducer\nuse Memo\nuse Callback\nuse Ref\nuse Layout Effect\nuse Im per ative Handle\nvirtual DOM\nre conciliation\ns usp ense\nerror Boundary\nuse Strict Mode\ninstance of\nend declare\nend for\nends witch\nend while\n","output_type":"stream"}],"execution_count":36},{"cell_type":"markdown","source":"## Llama 3.1-405B","metadata":{}},{"cell_type":"code","source":"for missword in misswords:\n    colored_code = tokenize_and_colorize(llama_tokenizer, missword)\n    print(colored_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T12:19:50.276361Z","iopub.execute_input":"2025-01-22T12:19:50.276708Z","iopub.status.idle":"2025-01-22T12:19:50.288349Z","shell.execute_reply.started":"2025-01-22T12:19:50.276678Z","shell.execute_reply":"2025-01-22T12:19:50.287537Z"}},"outputs":[{"name":"stdout","text":"non local\ns ynchronized\ntrans ient\nstrict fp\nfall through\ncomplex 64\ncomplex 128\nfloat 32\nuse Effect\nuse Context\nuse Reducer\nuse Memo\nuse Callback\nuse Ref\nuse Layout Effect\nuse Im per ative Handle\nvirtual DOM\nre conciliation\ns usp ense\nerror Boundary\nuse Strict Mode\ninstance of\nend declare\nend for\nends witch\nend while\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}