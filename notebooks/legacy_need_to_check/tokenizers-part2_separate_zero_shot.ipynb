{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10546507,"sourceType":"datasetVersion","datasetId":6525356},{"sourceId":10625817,"sourceType":"datasetVersion","datasetId":6579029},{"sourceId":10642913,"sourceType":"datasetVersion","datasetId":6589820}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nfrom itertools import islice\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom transformers import (\n    PreTrainedTokenizerFast,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:40:50.741417Z","iopub.execute_input":"2025-02-04T11:40:50.741719Z","iopub.status.idle":"2025-02-04T11:40:50.745885Z","shell.execute_reply.started":"2025-02-04T11:40:50.741696Z","shell.execute_reply":"2025-02-04T11:40:50.745199Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Upload tools","metadata":{}},{"cell_type":"code","source":"# deepseek_model_name = \"deepseek-ai/DeepSeek-V2-Lite\"\n# deepseek_model = AutoModelForCausalLM.from_pretrained(deepseek_model_name,\n#                                                       trust_remote_code=True)  \n\n\n# deepseek_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n# deepseek_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\n\n# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)  \n\n\n# athene_tokenizer = AutoTokenizer.from_pretrained(\"Nexusflow/Athene-V2-Chat\")\n# athene_model = AutoModelForCausalLM.from_pretrained(\"Nexusflow/Athene-V2-Chat\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T09:00:45.687788Z","iopub.execute_input":"2025-01-27T09:00:45.688221Z","iopub.status.idle":"2025-01-27T09:02:27.517430Z","shell.execute_reply.started":"2025-01-27T09:00:45.688189Z","shell.execute_reply":"2025-01-27T09:02:27.516806Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5b5a787175e4a91a595dd5c534c459d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed20142a506141f595b6d601e0a10e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"708dbdfc85394de68e44d8ecb60c6f8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ac48de397134ded820c733b8d4c96f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a73d9faf518e49e8aaafa115b98b4649"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# qwen_tokenizer_path = \"/kaggle/input/tokenizers/qwen_tokenizer.json\"\n# deepseek_tokenizer_path = \"/kaggle/input/tokenizers/deepseek_tokenizer.json\"\n# llama_tokenizer_path = \"/kaggle/input/tokenizers/llama_tokenizer.json\"\n\n# with open(qwen_tokenizer_path, 'r') as file:\n#     qwen_tokenizer = PreTrainedTokenizerFast(tokenizer_file=qwen_tokenizer_path)\n#     qwen = json.load(file)\n#     qwen_vocab = qwen['model']['vocab']\n# with open(deepseek_tokenizer_path, 'r') as file:\n#     deepseek_tokenizer = PreTrainedTokenizerFast(tokenizer_file=deepseek_tokenizer_path)\n#     deepseek = json.load(file)\n#     deepseek_vocab = deepseek['model']['vocab']\n# with open(llama_tokenizer_path, 'r') as file:\n#     llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=llama_tokenizer_path)\n#     llama = json.load(file)\n#     llama_vocab = llama['model']['vocab']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T08:56:05.001929Z","iopub.execute_input":"2025-01-27T08:56:05.002480Z","iopub.status.idle":"2025-01-27T08:56:05.551250Z","shell.execute_reply.started":"2025-01-27T08:56:05.002453Z","shell.execute_reply":"2025-01-27T08:56:05.550520Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"python_keywords = [\n    \"def\", \"class\", \"import\", \"from\", \"as\", \"if\", \"elif\", \"else\", \"while\", \n    \"for\", \"in\", \"break\", \"continue\", \"return\", \"yield\", \"try\", \"except\", \n    \"finally\", \"with\", \"assert\", \"lambda\", \"global\", \"nonlocal\", \"pass\", \n    \"raise\", \"True\", \"False\", \"None\", \"is\", \"not\"\n]\n\njava_keywords = [\n    \"class\", \"interface\", \"extends\", \"implements\", \"package\", \"import\", \"public\", \n    \"private\", \"protected\", \"static\", \"final\", \"abstract\", \"synchronized\", \n    \"volatile\", \"transient\", \"native\", \"strictfp\", \"void\", \"int\", \"double\", \n    \"float\", \"char\", \"boolean\", \"long\", \"short\", \"byte\", \"if\", \"else\", \"while\",\n    \"for\"\n]\n\n\ncpp_keywords = [\n    \"int\", \"float\", \"double\", \"char\", \"void\", \"bool\", \"short\", \"long\", \"signed\", \n    \"unsigned\", \"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \n    \"do\", \"break\", \"continue\", \"return\", \"goto\", \"class\", \"struct\", \"union\", \n    \"namespace\", \"using\", \"private\", \"protected\", \"public\"\n]\n\ngo_keywords = [\n    \"break\", \"case\", \"chan\", \"const\", \"continue\", \"default\", \"defer\", \"else\", \n    \"fallthrough\", \"for\", \"func\", \"go\", \"goto\", \"if\", \"import\", \"interface\", \n    \"map\", \"package\", \"range\", \"return\", \"select\", \"struct\", \"switch\", \"type\", \n    \"var\", \"bool\", \"byte\", \"complex64\", \"complex128\", \"float32\"\n]\n\nreact_keywords = [\n    \"React\", \"Component\", \"useState\", \"useEffect\", \"useContext\", \"useReducer\", \n    \"useMemo\", \"useCallback\", \"useRef\", \"useLayoutEffect\", \"useImperativeHandle\", \n    \"jsx\", \"props\", \"state\", \"context\", \"provider\", \"consumer\", \"hook\", \n    \"fragment\", \"key\", \"ref\", \"render\", \"virtualDOM\", \"reconciliation\", \n    \"portal\", \"suspense\", \"lazy\", \"errorBoundary\", \"children\", \"strictMode\"\n]\n\njavascript_keywords = [\n    \"var\", \"let\", \"const\", \"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \n    \"while\", \"do\", \"break\", \"continue\", \"return\", \"function\", \"class\", \n    \"extends\", \"constructor\", \"super\", \"import\", \"export\", \"try\",\n    \"catch\", \"finally\", \"throw\", \"this\", \"new\", \"delete\", \"instanceof\", \"typeof\"\n]\n\nrust_keywords = [\n    \"as\", \"break\", \"const\", \"continue\", \"crate\", \"else\", \"enum\", \"extern\", \n    \"false\", \"fn\", \"for\", \"if\", \"impl\", \"in\", \"let\", \"loop\", \"match\", \n    \"mod\", \"move\", \"mut\", \"pub\", \"ref\", \"return\", \"Self\", \"self\", \"static\", \n    \"struct\", \"super\", \"trait\", \"true\"\n]\n\nphp_keywords = [\n    \"abstract\", \"and\", \"array\", \"as\", \"break\", \"callable\", \"case\", \"catch\", \n    \"class\", \"clone\", \"const\", \"continue\", \"declare\", \"default\", \"do\", \"echo\", \n    \"else\", \"elseif\", \"empty\", \"enddeclare\", \"endfor\", \"endforeach\", \"endif\", \n    \"endswitch\", \"endwhile\", \"eval\", \"exit\", \"extends\", \"final\", \"finally\"\n]\n\nall_keywords = [python_keywords,\n                java_keywords,\n                cpp_keywords,\n                go_keywords,\n                react_keywords,\n                javascript_keywords,\n                rust_keywords,\n                php_keywords,]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:40:56.821602Z","iopub.execute_input":"2025-02-04T11:40:56.821880Z","iopub.status.idle":"2025-02-04T11:40:56.829835Z","shell.execute_reply.started":"2025-02-04T11:40:56.821857Z","shell.execute_reply":"2025-02-04T11:40:56.829105Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"misswords = ['nonlocal',\n             'synchronized',\n             'transient',\n             'strictfp',\n             'fallthrough',\n             'complex64',\n             'complex128',\n             'float32',\n             'useEffect',\n             'useContext',\n             'useReducer',\n             'useMemo',\n             'useCallback',\n             'useRef',\n             'useLayoutEffect',\n             'useImperativeHandle',\n             'virtualDOM',\n             'reconciliation',\n             'suspense',\n             'errorBoundary',\n             'useStrictMode',\n             'instanceof',\n             'enddeclare',\n             'endfor',\n             'endswitch',\n             'endwhile',\n            ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:40:57.380498Z","iopub.execute_input":"2025-02-04T11:40:57.380738Z","iopub.status.idle":"2025-02-04T11:40:57.384660Z","shell.execute_reply.started":"2025-02-04T11:40:57.380717Z","shell.execute_reply":"2025-02-04T11:40:57.383832Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Warm start","metadata":{}},{"cell_type":"code","source":"def get_token_probabilities(model, tokenizer, text, top_k=None):\n    \"\"\"\n    Get token probabilities from a GPT model for each position in the input text.\n\n    Args:\n        model: model instance\n        tokenizer: tokenizer instance\n        text: Input text to analyze\n        top_k: Optional, number of top probabilities to return for each position\n\n    Returns:\n        List of dictionaries containing token probabilities for each position\n    \"\"\"\n    # Tokenize input text\n    input_ids = tokenizer.encode(text, return_tensors='pt')\n\n    # Get model's raw predictions\n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits\n\n    # Convert logits to probabilities using softmax\n    probs = F.softmax(logits[0], dim=-1)\n\n    # Store results for each position\n    token_probs = []\n\n    # Analyze each position\n    for position in range(len(input_ids[0])):\n        position_probs = probs[position]\n\n        # Get token indices sorted by probability\n        sorted_indices = torch.argsort(position_probs, descending=True)\n\n        # Limit to top_k if specified\n        if top_k:\n            sorted_indices = sorted_indices[:top_k]\n\n        # Create dictionary of token:probability pairs\n        position_dict = {\n            'position': position,\n            'token': tokenizer.decode(input_ids[0][position]),\n            'probabilities': {\n                tokenizer.decode([idx.item()]): position_probs[idx].item()\n                for idx in sorted_indices\n            }\n        }\n\n        token_probs.append(position_dict)\n\n    return token_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T11:43:17.432023Z","iopub.execute_input":"2025-02-03T11:43:17.432299Z","iopub.status.idle":"2025-02-03T11:43:17.442150Z","shell.execute_reply.started":"2025-02-03T11:43:17.432272Z","shell.execute_reply":"2025-02-03T11:43:17.441517Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_token_probabilities_gpu(model, tokenizer, text, top_k=None):\n    \"\"\"\n    Get token probabilities from a GPT model for each position in the input text.\n\n    Args:\n        model: GPT model instance\n        tokenizer: GPT tokenizer instance\n        text: Input text to analyze\n        top_k: Optional, number of top probabilities to return for each position\n\n    Returns:\n        List of dictionaries containing token probabilities for each position\n    \"\"\"\n    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n\n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits\n\n    probs = F.softmax(logits[0], dim=-1)\n    token_probs = []\n\n    for position in range(len(input_ids[0])):\n        position_probs = probs[position]\n\n        sorted_indices = torch.argsort(position_probs, descending=True)\n        \n        if top_k:\n            sorted_indices = sorted_indices[:top_k]\n\n        position_dict = {\n            'position': position,\n            'token': tokenizer.decode(input_ids[0][position]),\n            'probabilities': {\n                tokenizer.decode([idx.item()]): position_probs[idx].item()\n                for idx in sorted_indices\n            }\n        }\n\n        token_probs.append(position_dict)\n\n    return token_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T11:59:43.607874Z","iopub.execute_input":"2025-02-03T11:59:43.608181Z","iopub.status.idle":"2025-02-03T11:59:43.613810Z","shell.execute_reply.started":"2025-02-03T11:59:43.608156Z","shell.execute_reply":"2025-02-03T11:59:43.613022Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Example usage\ndef analyze_text_gpu(text, model, tokenizer, top_k=50):\n    \"\"\"\n    Analyze token probabilities for a given text using specified GPT model.\n\n    Args:\n        text: Input text to analyze\n        model_name: Name of the GPT model to use\n        top_k: Number of top probabilities to show for each position\n    \"\"\"\n    # Load model and tokenizer\n    # model = GPT2LMHeadModel.from_pretrained(model_name)\n    # tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n\n    # Get token probabilities\n    results = get_token_probabilities_gpu(model, tokenizer, text, top_k)\n    \n    # Print results\n    # print(f\"Analysis of text: '{text}'\\n\")\n    # for pos_data in results[:5]:\n    #     print(f\"Position {pos_data['position']}: Token '{pos_data['token']}'\")\n    #     print(\"Top probabilities:\")\n    #     for token, prob in islice(pos_data['probabilities'].items(), 3):\n    #         print(f\"->{token}<-: {prob:.4f}\")\n    #     print()\n        \n    tokens_prob = {}\n    for i in range(len(results)):\n        given_token = results[i]['token']\n        tokens_prob[given_token] = {}\n        \n        for token, prob in islice(results[i]['probabilities'].items(), 3):\n            tokens_prob[given_token][token] = prob\n\n    return tokens_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T11:59:43.885679Z","iopub.execute_input":"2025-02-03T11:59:43.885962Z","iopub.status.idle":"2025-02-03T11:59:43.890895Z","shell.execute_reply.started":"2025-02-03T11:59:43.885939Z","shell.execute_reply":"2025-02-03T11:59:43.890017Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Example usage\ndef analyze_text(text, model, tokenizer, top_k=50):\n    \"\"\"\n    Analyze token probabilities for a given text using specified model.\n\n    Args:\n        text: Input text to analyze\n        model: Model to use\n        tokenizer: Tokenizer to use\n        top_k: Number of top probabilities to show for each position\n    \"\"\"\n\n    # Get token probabilities\n    results = get_token_probabilities(model, tokenizer, text, top_k)\n\n    tokens_prob = {}\n    for i in range(len(results)):\n        given_token = results[i]['token']\n        tokens_prob[given_token] = {}\n        \n        for token, prob in islice(results[i]['probabilities'].items(), 10):\n            tokens_prob[given_token][token] = prob\n\n    return tokens_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T11:43:17.442946Z","iopub.execute_input":"2025-02-03T11:43:17.443143Z","iopub.status.idle":"2025-02-03T11:43:17.455320Z","shell.execute_reply.started":"2025-02-03T11:43:17.443125Z","shell.execute_reply":"2025-02-03T11:43:17.454652Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"algorithms = pd.read_parquet(\"/kaggle/input/leetcode-tasks/train-00000-of-00001.parquet\")\n\n# algorithms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T12:01:51.326063Z","iopub.execute_input":"2025-02-03T12:01:51.326337Z","iopub.status.idle":"2025-02-03T12:01:52.057896Z","shell.execute_reply.started":"2025-02-03T12:01:51.326315Z","shell.execute_reply":"2025-02-03T12:01:52.056742Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"algorithms = algorithms[(algorithms[\"language\"] == \"python\") | (algorithms[\"language\"] == \"javascript\")]\nalgorithms = algorithms.drop(columns=[\n    'slug',\n    'difficulty',\n    'qwq',\n]).reset_index(drop=True)\n\nprint(algorithms[\"language\"].value_counts())\nalgorithms.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T12:01:52.059274Z","iopub.execute_input":"2025-02-03T12:01:52.059577Z","iopub.status.idle":"2025-02-03T12:01:52.095376Z","shell.execute_reply.started":"2025-02-03T12:01:52.059553Z","shell.execute_reply":"2025-02-03T12:01:52.094670Z"}},"outputs":[{"name":"stdout","text":"language\npython        397\njavascript    389\nName: count, dtype: int64\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"     id                                             title  \\\n0  1568       Minimum Number of Days to Disconnect Island   \n1  1714    Sum Of Special Evenly-Spaced Elements In Array   \n2  1326  Minimum Number of Taps to Open to Water a Garden   \n\n                                             content  \\\n0  You are given an `m x n` binary grid `grid` wh...   \n1  You are given a **0-indexed** integer array `n...   \n2  There is a one-dimensional garden on the x-axi...   \n\n                                            solution    language  \n0  ```python\\ndef pseudoPalindromicPaths(root, cn...      python  \n1  ```javascript\\nfunction reorderSpaces(text) {\\...  javascript  \n2  ```javascript\\nfunction sumOfFlooredPairs(nums...  javascript  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>content</th>\n      <th>solution</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1568</td>\n      <td>Minimum Number of Days to Disconnect Island</td>\n      <td>You are given an `m x n` binary grid `grid` wh...</td>\n      <td>```python\\ndef pseudoPalindromicPaths(root, cn...</td>\n      <td>python</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1714</td>\n      <td>Sum Of Special Evenly-Spaced Elements In Array</td>\n      <td>You are given a **0-indexed** integer array `n...</td>\n      <td>```javascript\\nfunction reorderSpaces(text) {\\...</td>\n      <td>javascript</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1326</td>\n      <td>Minimum Number of Taps to Open to Water a Garden</td>\n      <td>There is a one-dimensional garden on the x-axi...</td>\n      <td>```javascript\\nfunction sumOfFlooredPairs(nums...</td>\n      <td>javascript</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Check on specific programming words","metadata":{}},{"cell_type":"code","source":"from typing import List\n\ndef warm_start(words: List[str], model, tokenizer):\n    words = ' '.join(words)\n    result = analyze_text(words, model, tokenizer)\n\n    return result\n\ndef warm_start_gpu(words: List[str], model, tokenizer):\n    words = ' '.join(words)\n    result = analyze_text_gpu(words, model, tokenizer)\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T11:59:49.886917Z","iopub.execute_input":"2025-02-03T11:59:49.887297Z","iopub.status.idle":"2025-02-03T11:59:49.893550Z","shell.execute_reply.started":"2025-02-03T11:59:49.887268Z","shell.execute_reply":"2025-02-03T11:59:49.892673Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### DeepSeek-R1-Distill-Qwen-7B","metadata":{}},{"cell_type":"code","source":"deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\ndeepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"deepseek-r1-qwen-7b.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DeepSeek-R1-Distill-Qwen-1.5B","metadata":{}},{"cell_type":"code","source":"device = \"cuda\"\n\ndeepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\ndeepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\ndeepseek_r1_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:48:32.261156Z","iopub.execute_input":"2025-02-04T11:48:32.261563Z","iopub.status.idle":"2025-02-04T11:50:17.755966Z","shell.execute_reply.started":"2025-02-04T11:48:32.261533Z","shell.execute_reply":"2025-02-04T11:50:17.755154Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79680ee05413421b94a704aa80998c30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b1a78a614d64a6880303620ec7d4e44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3041005652344ac985ee64a375bddf16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf5f965a1cfd4b46b202469089b0bc29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36255c971c55459ba03ee11d4255d2b5"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"#### Previous way","metadata":{}},{"cell_type":"code","source":"with open(\"deepseek-r1-qwen-1500M.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start_gpu(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T11:59:55.134169Z","iopub.execute_input":"2025-02-03T11:59:55.134448Z","iopub.status.idle":"2025-02-03T11:59:56.955141Z","shell.execute_reply.started":"2025-02-03T11:59:55.134427Z","shell.execute_reply":"2025-02-03T11:59:56.954405Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"with open(\"deepseek-r1-qwen-1500M-missed-tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    warm_start_result = warm_start_gpu(misswords, deepseek_r1_model, deepseek_r1_tokenizer)\n    results[f\"misswords\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T12:00:06.406593Z","iopub.execute_input":"2025-02-03T12:00:06.406898Z","iopub.status.idle":"2025-02-03T12:00:06.669114Z","shell.execute_reply.started":"2025-02-03T12:00:06.406875Z","shell.execute_reply":"2025-02-03T12:00:06.668482Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def answer_question(prompt, model, tokenizer):\n    \"\"\"\n    Generates an answer based on the given context using a causal language model.\n\n    Args:\n        question (str): The question to be answered.\n        context (str): The context in which to find the answer.\n        model: The causal language model.\n        tokenizer: The tokenizer for the model.\n\n    Returns:\n        str: The generated answer.\n    \"\"\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=512)\n\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T12:01:33.606883Z","iopub.execute_input":"2025-02-03T12:01:33.607192Z","iopub.status.idle":"2025-02-03T12:01:33.612053Z","shell.execute_reply.started":"2025-02-03T12:01:33.607169Z","shell.execute_reply":"2025-02-03T12:01:33.611099Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# tasks = pd.read_parquet(\"/kaggle/input/tokenizers-python-dataset/train-00000-of-00002.parquet\")\n\n# tasks.head(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:50:39.619724Z","iopub.execute_input":"2025-02-04T11:50:39.620002Z","iopub.status.idle":"2025-02-04T11:50:39.623343Z","shell.execute_reply.started":"2025-02-04T11:50:39.619979Z","shell.execute_reply":"2025-02-04T11:50:39.622494Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"df_solutions = pd.DataFrame()\ntokens_to_give = \"\"\n\n# for i in range(len(algorithms)):\nfor i in range(15):\n    if i%5 == 0:\n        print(i)\n\n    task = algorithms[\"content\"][i]\n    task_id = algorithms[\"id\"][i]\n    task_name = algorithms[\"title\"][i]\n    language = algorithms[\"language\"][i]\n    prompt = f\"Solve this programming task, you need to return only program code in {language}. Task: {task}\"\n\n    solution = answer_question(prompt, deepseek_r1_model, deepseek_r1_tokenizer)\n\n    df_solutions.loc[i, \"task_id\"] = task_id\n    df_solutions.loc[i, \"task_name\"] = task_name\n    df_solutions.loc[i, \"task\"] = task\n    df_solutions.loc[i, \"language\"] = language\n    df_solutions.loc[i, \"prompt\"] = prompt\n    df_solutions.loc[i, \"solution\"] = solution\n\ndf_solutions.to_csv(\"deepseek_r1_distill_qwen_1500M.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T12:01:59.606502Z","iopub.execute_input":"2025-02-03T12:01:59.606832Z","iopub.status.idle":"2025-02-03T12:06:31.202311Z","shell.execute_reply.started":"2025-02-03T12:01:59.606808Z","shell.execute_reply":"2025-02-03T12:06:31.201563Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"5\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n\nfor i in range(1):\n    task = algorithms[\"content\"][i]\n    task_id = algorithms[\"id\"][i]\n    task_name = algorithms[\"title\"][i]\n    language = algorithms[\"language\"][i]\n    prompt = f\"Solve this programming task, you need to return only program code in {language}. Task: {task}\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n\n    text = deepseek_r1_tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n    \n    generated_ids = deepseek_r1_model.generate(\n        **model_inputs,\n        max_new_tokens=2048\n    )\n    generated_ids = [\n        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n    ]\n\n    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    df_solutions.loc[i, \"task_id\"] = task_id\n    df_solutions.loc[i, \"task_name\"] = task_name\n    df_solutions.loc[i, \"task\"] = task\n    df_solutions.loc[i, \"language\"] = language\n    df_solutions.loc[i, \"prompt\"] = prompt\n    df_solutions.loc[i, \"solution\"] = response\n    print(i)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:20:34.243307Z","iopub.execute_input":"2025-02-03T08:20:34.243624Z","iopub.status.idle":"2025-02-03T08:21:55.999159Z","shell.execute_reply.started":"2025-02-03T08:20:34.243600Z","shell.execute_reply":"2025-02-03T08:21:55.998280Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"0\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"# df_solutions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:50:31.460493Z","iopub.execute_input":"2025-02-04T11:50:31.460811Z","iopub.status.idle":"2025-02-04T11:50:31.464212Z","shell.execute_reply.started":"2025-02-04T11:50:31.460782Z","shell.execute_reply":"2025-02-04T11:50:31.463346Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# df_solutions[\"solution\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:50:27.620567Z","iopub.execute_input":"2025-02-04T11:50:27.621137Z","iopub.status.idle":"2025-02-04T11:50:27.624682Z","shell.execute_reply.started":"2025-02-04T11:50:27.621110Z","shell.execute_reply":"2025-02-04T11:50:27.623690Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"#### New way","metadata":{}},{"cell_type":"code","source":"def warm_start(word: str, model, tokenizer):\n    \"\"\"\n    Process a single word and get probabilities of the next 10 tokens.\n    \"\"\"\n    return analyze_text(word, model, tokenizer)\n\ndef analyze_text(word: str, model, tokenizer, top_k=10):\n    \"\"\"\n    Analyze token probabilities for a given word.\n    \"\"\"\n    results = get_token_probabilities(model, tokenizer, word, top_k)\n    \n    tokens_prob = {}\n    if results:\n        given_token = results[0]['token']\n        tokens_prob[given_token] = results[0]['probabilities']\n    \n    return tokens_prob\n\ndef get_token_probabilities(model, tokenizer, word: str, top_k=10):\n    \"\"\"\n    Get token probabilities for the next tokens based on a single input word.\n    \"\"\"\n    # Tokenize input word\n    input_ids = tokenizer.encode(word, return_tensors='pt').to(device)\n    \n    # Get model's raw predictions\n    with torch.no_grad():\n        outputs = model(input_ids)\n        logits = outputs.logits\n    \n    # Take the last token's logits for next token prediction\n    last_token_logits = logits[0, -1, :]\n    \n    # Convert logits to probabilities using softmax\n    probs = F.softmax(last_token_logits, dim=-1)\n    \n    # Get token indices sorted by probability\n    sorted_indices = torch.argsort(probs, descending=True)[:top_k]\n    \n    # Create dictionary of token:probability pairs\n    return [{\n        'token': word,\n        'probabilities': {\n            tokenizer.decode([idx.item()]): probs[idx].item()\n            for idx in sorted_indices\n        }\n    }]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:52:43.220679Z","iopub.execute_input":"2025-02-04T11:52:43.220960Z","iopub.status.idle":"2025-02-04T11:52:43.227958Z","shell.execute_reply.started":"2025-02-04T11:52:43.220939Z","shell.execute_reply":"2025-02-04T11:52:43.226911Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"with open(\"deepseek-r1-qwen-1500M.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n\n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        results[f\"language_{i}\"] = {}\n        for j in range(len(keywords)):\n            warm_start_result = warm_start(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n            results[f\"language_{i}\"][j] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:58:41.300316Z","iopub.execute_input":"2025-02-04T11:58:41.300630Z","iopub.status.idle":"2025-02-04T11:58:51.213374Z","shell.execute_reply.started":"2025-02-04T11:58:41.300606Z","shell.execute_reply":"2025-02-04T11:58:51.212709Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"with open(\"deepseek-r1-qwen-1500M-missed-tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    results[f\"misswords\"] = {}\n    \n    for i in range(len(misswords)):\n        warm_start_result = warm_start(misswords[i], deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"misswords\"][i] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T12:26:53.460887Z","iopub.execute_input":"2025-02-04T12:26:53.461213Z","iopub.status.idle":"2025-02-04T12:26:54.646930Z","shell.execute_reply.started":"2025-02-04T12:26:53.461189Z","shell.execute_reply":"2025-02-04T12:26:54.646216Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"### DeepSeek-R1-Distill-Llama-8B ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T08:15:47.841963Z","iopub.execute_input":"2025-02-03T08:15:47.842242Z","iopub.status.idle":"2025-02-03T08:15:47.847306Z","shell.execute_reply.started":"2025-02-03T08:15:47.842221Z","shell.execute_reply":"2025-02-03T08:15:47.846625Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"'<think>\\nOkay, I need to solve this programming problem. Let me read the problem statement carefully.\\n\\n'"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\ndeepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n\nwith open(\"deepseek-r1-llama-8b.txt\", \"w\", encoding=\"utf-8\") as f:\n    results = {}\n    \n    for i in range(len(all_keywords)):\n        keywords = all_keywords[i]\n        warm_start_result = warm_start(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n        results[f\"language_{i}\"] = warm_start_result\n\n    json.dump(results, f, ensure_ascii=False, indent=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Miss words in tokenizers","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:31:14.131900Z","iopub.execute_input":"2025-01-22T11:31:14.132346Z","iopub.status.idle":"2025-01-22T11:31:14.136957Z","shell.execute_reply.started":"2025-01-22T11:31:14.132309Z","shell.execute_reply":"2025-01-22T11:31:14.135923Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import random\nfrom termcolor import colored\n\ncolors = ['red', 'green', 'yellow', 'blue']\n\ndef tokenize_and_colorize(tokenizer, code):\n    tokens = tokenizer.tokenize(code)\n    colored_code = []\n    last_color = None\n\n    for tok in tokens:\n        new_color = random.choice([color for color in colors if color != last_color])\n        last_color = new_color\n        colored_code.append(colored(tok, new_color))\n\n    return ' '.join(colored_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:46:37.883595Z","iopub.execute_input":"2025-01-22T11:46:37.883912Z","iopub.status.idle":"2025-01-22T11:46:37.888916Z","shell.execute_reply.started":"2025-01-22T11:46:37.883889Z","shell.execute_reply":"2025-01-22T11:46:37.887912Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"## Deepseek r1","metadata":{}},{"cell_type":"code","source":"for missword in misswords:\n    colored_code = tokenize_and_colorize(deepseek_tokenizer, missword)\n    print(colored_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T11:46:40.549551Z","iopub.execute_input":"2025-01-22T11:46:40.549892Z","iopub.status.idle":"2025-01-22T11:46:40.564981Z","shell.execute_reply.started":"2025-01-22T11:46:40.549860Z","shell.execute_reply":"2025-01-22T11:46:40.564079Z"}},"outputs":[{"name":"stdout","text":"non local\ns ynchronized\ntrans ient\nstrict fp\nfall through\ncomplex 6 4\ncomplex 1 2 8\nfloat 3 2\nuse Effect\nuse Context\nuse Reducer\nuse Memo\nuse Callback\nuse Ref\nuse Layout Effect\nuse Im per ative Handle\nvirtual DOM\nre conciliation\ns usp ense\nerror Boundary\nuse Strict Mode\ninstance of\nend declare\nend for\nends witch\nend while\n","output_type":"stream"}],"execution_count":36}]}