{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import islice\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas transformers pyarrow 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T09:00:45.688221Z",
     "iopub.status.busy": "2025-01-27T09:00:45.687788Z",
     "iopub.status.idle": "2025-01-27T09:02:27.517430Z",
     "shell.execute_reply": "2025-01-27T09:02:27.516806Z",
     "shell.execute_reply.started": "2025-01-27T09:00:45.688189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b5a787175e4a91a595dd5c534c459d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed20142a506141f595b6d601e0a10e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708dbdfc85394de68e44d8ecb60c6f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ac48de397134ded820c733b8d4c96f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73d9faf518e49e8aaafa115b98b4649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# deepseek_model_name = \"deepseek-ai/DeepSeek-V2-Lite\"\n",
    "# deepseek_model = AutoModelForCausalLM.from_pretrained(deepseek_model_name,\n",
    "#                                                       trust_remote_code=True)  \n",
    "\n",
    "\n",
    "# deepseek_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "# deepseek_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "\n",
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1\", trust_remote_code=True)  \n",
    "\n",
    "\n",
    "# athene_tokenizer = AutoTokenizer.from_pretrained(\"Nexusflow/Athene-V2-Chat\")\n",
    "# athene_model = AutoModelForCausalLM.from_pretrained(\"Nexusflow/Athene-V2-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_tokenizer_path = \"/kaggle/input/tokenizers/qwen_tokenizer.json\"\n",
    "# deepseek_tokenizer_path = \"/kaggle/input/tokenizers/deepseek_tokenizer.json\"\n",
    "# llama_tokenizer_path = \"/kaggle/input/tokenizers/llama_tokenizer.json\"\n",
    "\n",
    "# with open(qwen_tokenizer_path, 'r') as file:\n",
    "#     qwen_tokenizer = PreTrainedTokenizerFast(tokenizer_file=qwen_tokenizer_path)\n",
    "#     qwen = json.load(file)\n",
    "#     qwen_vocab = qwen['model']['vocab']\n",
    "# with open(deepseek_tokenizer_path, 'r') as file:\n",
    "#     deepseek_tokenizer = PreTrainedTokenizerFast(tokenizer_file=deepseek_tokenizer_path)\n",
    "#     deepseek = json.load(file)\n",
    "#     deepseek_vocab = deepseek['model']['vocab']\n",
    "# with open(llama_tokenizer_path, 'r') as file:\n",
    "#     llama_tokenizer = PreTrainedTokenizerFast(tokenizer_file=llama_tokenizer_path)\n",
    "#     llama = json.load(file)\n",
    "#     llama_vocab = llama['model']['vocab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated from ChatGPT\n",
    "\n",
    "# python_keywords = [\n",
    "#     \"def\", \"class\", \"import\", \"from\", \"as\", \"if\", \"elif\", \"else\", \"while\", \n",
    "#     \"for\", \"in\", \"break\", \"continue\", \"return\", \"yield\", \"try\", \"except\", \n",
    "#     \"finally\", \"with\", \"assert\", \"lambda\", \"global\", \"nonlocal\", \"pass\", \n",
    "#     \"raise\", \"True\", \"False\", \"None\", \"is\", \"not\"\n",
    "# ]\n",
    "\n",
    "# java_keywords = [\n",
    "#     \"class\", \"interface\", \"extends\", \"implements\", \"package\", \"import\", \"public\", \n",
    "#     \"private\", \"protected\", \"static\", \"final\", \"abstract\", \"synchronized\", \n",
    "#     \"volatile\", \"transient\", \"native\", \"strictfp\", \"void\", \"int\", \"double\", \n",
    "#     \"float\", \"char\", \"boolean\", \"long\", \"short\", \"byte\", \"if\", \"else\", \"while\",\n",
    "#     \"for\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# cpp_keywords = [\n",
    "#     \"int\", \"float\", \"double\", \"char\", \"void\", \"bool\", \"short\", \"long\", \"signed\", \n",
    "#     \"unsigned\", \"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \"while\", \n",
    "#     \"do\", \"break\", \"continue\", \"return\", \"goto\", \"class\", \"struct\", \"union\", \n",
    "#     \"namespace\", \"using\", \"private\", \"protected\", \"public\"\n",
    "# ]\n",
    "\n",
    "# go_keywords = [\n",
    "#     \"break\", \"case\", \"chan\", \"const\", \"continue\", \"default\", \"defer\", \"else\", \n",
    "#     \"fallthrough\", \"for\", \"func\", \"go\", \"goto\", \"if\", \"import\", \"interface\", \n",
    "#     \"map\", \"package\", \"range\", \"return\", \"select\", \"struct\", \"switch\", \"type\", \n",
    "#     \"var\", \"bool\", \"byte\", \"complex64\", \"complex128\", \"float32\"\n",
    "# ]\n",
    "\n",
    "# react_keywords = [\n",
    "#     \"React\", \"Component\", \"useState\", \"useEffect\", \"useContext\", \"useReducer\", \n",
    "#     \"useMemo\", \"useCallback\", \"useRef\", \"useLayoutEffect\", \"useImperativeHandle\", \n",
    "#     \"jsx\", \"props\", \"state\", \"context\", \"provider\", \"consumer\", \"hook\", \n",
    "#     \"fragment\", \"key\", \"ref\", \"render\", \"virtualDOM\", \"reconciliation\", \n",
    "#     \"portal\", \"suspense\", \"lazy\", \"errorBoundary\", \"children\", \"strictMode\"\n",
    "# ]\n",
    "\n",
    "# javascript_keywords = [\n",
    "#     \"var\", \"let\", \"const\", \"if\", \"else\", \"switch\", \"case\", \"default\", \"for\", \n",
    "#     \"while\", \"do\", \"break\", \"continue\", \"return\", \"function\", \"class\", \n",
    "#     \"extends\", \"constructor\", \"super\", \"import\", \"export\", \"try\",\n",
    "#     \"catch\", \"finally\", \"throw\", \"this\", \"new\", \"delete\", \"instanceof\", \"typeof\"\n",
    "# ]\n",
    "\n",
    "# rust_keywords = [\n",
    "#     \"as\", \"break\", \"const\", \"continue\", \"crate\", \"else\", \"enum\", \"extern\", \n",
    "#     \"false\", \"fn\", \"for\", \"if\", \"impl\", \"in\", \"let\", \"loop\", \"match\", \n",
    "#     \"mod\", \"move\", \"mut\", \"pub\", \"ref\", \"return\", \"Self\", \"self\", \"static\", \n",
    "#     \"struct\", \"super\", \"trait\", \"true\"\n",
    "# ]\n",
    "\n",
    "# php_keywords = [\n",
    "#     \"abstract\", \"and\", \"array\", \"as\", \"break\", \"callable\", \"case\", \"catch\", \n",
    "#     \"class\", \"clone\", \"const\", \"continue\", \"declare\", \"default\", \"do\", \"echo\", \n",
    "#     \"else\", \"elseif\", \"empty\", \"enddeclare\", \"endfor\", \"endforeach\", \"endif\", \n",
    "#     \"endswitch\", \"endwhile\", \"eval\", \"exit\", \"extends\", \"final\", \"finally\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# From Documentation\n",
    "python_keywords = [\n",
    "    'False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', \n",
    "    'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally',\n",
    "    'for', 'from', 'global', 'if',  'import', 'in', 'is', 'lambda',\n",
    "    'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while',\n",
    "    'with', 'yield',\n",
    "]\n",
    "java_keywords = [\n",
    "    'abstract', 'assert', 'boolean', 'break', 'byte', 'case', 'catch',\n",
    "    'char', 'class', 'continue', 'default', 'do', 'double', 'else',\n",
    "    'enum', 'extends', 'final', 'finally', 'float', 'for', 'if',\n",
    "    'implements', 'import', 'instanceof', 'int', 'interface', 'long',\n",
    "    'native', 'new', 'null', 'package', 'private', 'protected',\n",
    "    'public', 'return', 'short', 'static', 'strictfp', 'super',\n",
    "    'switch', 'synchronized', 'this', 'throw', 'throws', 'transient',\n",
    "    'try', 'void', 'volatile', 'while', 'sealed', 'permits',\n",
    "]\n",
    "cpp_keywords = [\n",
    "    'alignas', 'alignof', 'and', 'and_eq', 'asm', 'auto', 'bitand',\n",
    "    'bitor', 'bool', 'break', 'case', 'catch', 'char', 'char8_t',\n",
    "    'char16_t', 'char32_t', 'class', 'compl', 'concept', 'const',\n",
    "    'consteval', 'constexpr', 'constinit', 'const_cast', 'continue',\n",
    "    'co_await', 'co_return', 'co_yield', 'decltype', 'default',\n",
    "    'delete', 'do', 'double', 'dynamic_cast', 'else', 'enum',\n",
    "    'explicit', 'export', 'extern', 'false', 'final', 'float',\n",
    "    'for', 'friend', 'goto', 'if', 'inline', 'int', 'long',\n",
    "    'mutable', 'namespace', 'new', 'noexcept', 'not', 'not_eq',\n",
    "    'nullptr', 'operator', 'or', 'or_eq', 'private', 'protected',\n",
    "    'public', 'register', 'reinterpret_cast', 'requires', 'return',\n",
    "    'short', 'signed', 'sizeof', 'static', 'static_assert',\n",
    "    'static_cast', 'struct', 'switch', 'template', 'this',\n",
    "    'thread_local', 'throw', 'true', 'try', 'typedef', 'typeid',\n",
    "    'typename', 'union', 'unsigned', 'using', 'virtual', 'void',\n",
    "    'volatile', 'wchar_t', 'while', 'xor', 'xor_eq',\n",
    "]\n",
    "\n",
    "go_keywords = [\n",
    "    \"break\", \"case\", \"chan\", \"const\", \"continue\", \"default\", \"defer\",\n",
    "    \"else\", \"fallthrough\", \"for\", \"func\", \"go\", \"goto\", \"if\", \"import\",\n",
    "    \"interface\", \"map\", \"package\", \"range\", \"return\", \"select\",\n",
    "    \"struct\", \"switch\", \"type\", \"var\",\n",
    "]\n",
    "\n",
    "react_keywords = [\n",
    "    \"React\", \"Component\", \"useState\", \"useEffect\", \"useContext\", \"useReducer\", \n",
    "    \"useMemo\", \"useCallback\", \"useRef\", \"useLayoutEffect\", \"useImperativeHandle\", \n",
    "    \"jsx\", \"props\", \"state\", \"context\", \"provider\", \"consumer\", \"hook\", \n",
    "    \"fragment\", \"key\", \"ref\", \"render\", \"virtualDOM\", \"reconciliation\", \n",
    "    \"portal\", \"suspense\", \"lazy\", \"errorBoundary\", \"children\", \"strictMode\",\n",
    "]\n",
    "\n",
    "\n",
    "javascript_keywords = [\n",
    "    'await', 'break', 'case', 'catch', 'class', 'const', 'continue',\n",
    "    'debugger', 'default', 'delete', 'do', 'else', 'enum', 'export',\n",
    "    'extends', 'false', 'finally', 'for', 'function', 'if', 'implements',\n",
    "    'import', 'in', 'instanceof', 'interface', 'let', 'new', 'null',\n",
    "    'package', 'private', 'protected', 'public', 'return', 'super',\n",
    "    'switch', 'static', 'this', 'throw', 'try', 'true', 'typeof',\n",
    "    'var', 'void', 'while', 'with', 'yield',\n",
    "]\n",
    "\n",
    "rust_keywords = [\n",
    "    \"as\", \"break\", \"const\", \"continue\", \"crate\", \"else\", \"enum\", \"extern\", \n",
    "    \"false\", \"fn\", \"for\", \"if\", \"impl\", \"in\", \"let\", \"loop\", \"match\", \n",
    "    \"mod\", \"move\", \"mut\", \"pub\", \"ref\", \"return\", \"Self\", \"self\", \"static\", \n",
    "    \"struct\", \"super\", \"trait\", \"true\", \"type\", \"unsafe\", \"use\", \"where\", \"while\", \"async\", \"await\", \"dyn\", \"abstract\", \"become\", \"box\", \"do\", \"final\", \"macro\", \"override\", \"priv\", \"typeof\", \"unsized\", \"virtual\", \"yield\", \"try\"\n",
    "]\n",
    "\n",
    "php_keywords = [\n",
    "    \"abstract\", \"and\", \"as\", \"break\", \"callable\", \"case\", \"catch\", \n",
    "    \"class\", \"clone\", \"const\", \"continue\", \"declare\", \"default\", \"do\", \"echo\", \n",
    "    \"else\", \"elseif\", \"enddeclare\", \"endfor\", \"endforeach\", \"endif\", \n",
    "    \"endswitch\", \"endwhile\", \"extends\", \"final\", \"finally\", \"fn\", \"for\",\n",
    "    \"foreach\", \"function\", \"global\", \"goto\", \"if\", \"implements\", \"include\",\n",
    "    \"include_once\", \"instanceof\", \"insteadof\", \"interface\", \"match\",\n",
    "    \"namespace\", \"new\", \"or\", \"print\", \"private\", \"protected\", \"public\",\n",
    "    \"readonly\", \"require\", \"require_once\", \"return\", \"static\", \"switch\",\n",
    "    \"throw\", \"trait\", \"try\", \"use\", \"var\", \"while\", \"xor\", \"yield\",\n",
    "    \"yield from\",\n",
    "]\n",
    "\n",
    "all_keywords = [python_keywords,\n",
    "                java_keywords,\n",
    "                cpp_keywords,\n",
    "                go_keywords,\n",
    "                react_keywords,\n",
    "                javascript_keywords,\n",
    "                rust_keywords,\n",
    "                php_keywords,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "misswords = ['nonlocal',\n",
    "             'synchronized',\n",
    "             'transient',\n",
    "             'strictfp',\n",
    "             'fallthrough',\n",
    "             'complex64',\n",
    "             'complex128',\n",
    "             'float32',\n",
    "             'useEffect',\n",
    "             'useContext',\n",
    "             'useReducer',\n",
    "             'useMemo',\n",
    "             'useCallback',\n",
    "             'useRef',\n",
    "             'useLayoutEffect',\n",
    "             'useImperativeHandle',\n",
    "             'virtualDOM',\n",
    "             'reconciliation',\n",
    "             'suspense',\n",
    "             'errorBoundary',\n",
    "             'useStrictMode',\n",
    "             'instanceof',\n",
    "             'enddeclare',\n",
    "             'endfor',\n",
    "             'endswitch',\n",
    "             'endwhile',\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Functions to take the sequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_probabilities_sequence(model, tokenizer, text, top_k=None):\n",
    "    \"\"\"\n",
    "    Get token probabilities from a GPT model for each position in the input text.\n",
    "\n",
    "    Args:\n",
    "        model: GPT model instance\n",
    "        tokenizer: GPT tokenizer instance\n",
    "        text: Input text to analyze\n",
    "        top_k: Optional, number of top probabilities to return for each position\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing token probabilities for each position\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probs = F.softmax(logits[0], dim=-1)\n",
    "    token_probs = []\n",
    "\n",
    "    for position in range(len(input_ids[0])):\n",
    "        position_probs = probs[position]\n",
    "\n",
    "        sorted_indices = torch.argsort(position_probs, descending=True)\n",
    "        \n",
    "        if top_k:\n",
    "            sorted_indices = sorted_indices[:top_k]\n",
    "\n",
    "        position_dict = {\n",
    "            'position': position,\n",
    "            'token': tokenizer.decode(input_ids[0][position]),\n",
    "            'probabilities': {\n",
    "                tokenizer.decode([idx.item()]): position_probs[idx].item()\n",
    "                for idx in sorted_indices\n",
    "            }\n",
    "        }\n",
    "\n",
    "        token_probs.append(position_dict)\n",
    "\n",
    "    return token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def analyze_text_sequence(text, model, tokenizer, top_k=50):\n",
    "    \"\"\"\n",
    "    Analyze token probabilities for a given text using specified GPT model.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to analyze\n",
    "        model_name: Name of the GPT model to use\n",
    "        top_k: Number of top probabilities to show for each position\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    # model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    # tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Get token probabilities\n",
    "    results = get_token_probabilities_sequence(model, tokenizer, text, top_k)\n",
    "    \n",
    "    # Print results\n",
    "    # print(f\"Analysis of text: '{text}'\\n\")\n",
    "    # for pos_data in results[:5]:\n",
    "    #     print(f\"Position {pos_data['position']}: Token '{pos_data['token']}'\")\n",
    "    #     print(\"Top probabilities:\")\n",
    "    #     for token, prob in islice(pos_data['probabilities'].items(), 3):\n",
    "    #         print(f\"->{token}<-: {prob:.4f}\")\n",
    "    #     print()\n",
    "        \n",
    "    tokens_prob = {}\n",
    "    for i in range(len(results)):\n",
    "        given_token = results[i]['token']\n",
    "        tokens_prob[given_token] = {}\n",
    "        \n",
    "        for token, prob in islice(results[i]['probabilities'].items(), 3):\n",
    "            tokens_prob[given_token][token] = prob\n",
    "\n",
    "    return tokens_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def warm_start_sequence(words: List[str], model, tokenizer):\n",
    "    words = ' '.join(words)\n",
    "    result = analyze_text_sequence(words, model, tokenizer)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ALgorithms dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = pd.read_parquet(\"train-00000-of-00001.parquet\")\n",
    "\n",
    "# algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language\n",
      "python        397\n",
      "javascript    389\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>solution</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1568</td>\n",
       "      <td>Minimum Number of Days to Disconnect Island</td>\n",
       "      <td>You are given an `m x n` binary grid `grid` wh...</td>\n",
       "      <td>```python\\ndef pseudoPalindromicPaths(root, cn...</td>\n",
       "      <td>python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1714</td>\n",
       "      <td>Sum Of Special Evenly-Spaced Elements In Array</td>\n",
       "      <td>You are given a **0-indexed** integer array `n...</td>\n",
       "      <td>```javascript\\nfunction reorderSpaces(text) {\\...</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1326</td>\n",
       "      <td>Minimum Number of Taps to Open to Water a Garden</td>\n",
       "      <td>There is a one-dimensional garden on the x-axi...</td>\n",
       "      <td>```javascript\\nfunction sumOfFlooredPairs(nums...</td>\n",
       "      <td>javascript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                             title  \\\n",
       "0  1568       Minimum Number of Days to Disconnect Island   \n",
       "1  1714    Sum Of Special Evenly-Spaced Elements In Array   \n",
       "2  1326  Minimum Number of Taps to Open to Water a Garden   \n",
       "\n",
       "                                             content  \\\n",
       "0  You are given an `m x n` binary grid `grid` wh...   \n",
       "1  You are given a **0-indexed** integer array `n...   \n",
       "2  There is a one-dimensional garden on the x-axi...   \n",
       "\n",
       "                                            solution    language  \n",
       "0  ```python\\ndef pseudoPalindromicPaths(root, cn...      python  \n",
       "1  ```javascript\\nfunction reorderSpaces(text) {\\...  javascript  \n",
       "2  ```javascript\\nfunction sumOfFlooredPairs(nums...  javascript  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithms = algorithms[(algorithms[\"language\"] == \"python\") | (algorithms[\"language\"] == \"javascript\")]\n",
    "algorithms = algorithms.drop(columns=[\n",
    "    'slug',\n",
    "    'difficulty',\n",
    "    'qwq',\n",
    "]).reset_index(drop=True)\n",
    "\n",
    "print(algorithms[\"language\"].value_counts())\n",
    "algorithms.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only one token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вместо top_k=10 использовать top_k=len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подать на вход: \"Ты опытный программист, ты пишешь свою лучшую программу: ```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warm_start_token(word: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Process a single word and get probabilities of the next 10 tokens.\n",
    "    \"\"\"\n",
    "    return analyze_text_token(word, model, tokenizer)\n",
    "\n",
    "def analyze_text_token(word: str, model, tokenizer, top_k=10):\n",
    "    \"\"\"\n",
    "    Analyze token probabilities for a given word.\n",
    "    \"\"\"\n",
    "    results = get_token_probabilities_token(model, tokenizer, word, top_k)\n",
    "    \n",
    "    tokens_prob = {}\n",
    "    if results:\n",
    "        given_token = results[0]['token']\n",
    "        tokens_prob[given_token] = results[0]['probabilities']\n",
    "    \n",
    "    return tokens_prob\n",
    "\n",
    "def get_token_probabilities_token(model, tokenizer, word: str, top_k=10):\n",
    "    \"\"\"\n",
    "    Get token probabilities for the next tokens based on a single input word.\n",
    "    \"\"\"\n",
    "    # Tokenize input word\n",
    "    input_ids = tokenizer.encode(word, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Get model's raw predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Take the last token's logits for next token prediction\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    \n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = F.softmax(last_token_logits, dim=-1)\n",
    "    \n",
    "    # Get token indices sorted by probability\n",
    "    sorted_indices = torch.argsort(probs, descending=True)[:top_k]\n",
    "    \n",
    "    # Create dictionary of token:probability pairs\n",
    "    return [{\n",
    "        'token': word,\n",
    "        'probabilities': {\n",
    "            tokenizer.decode([idx.item()]): probs[idx].item()\n",
    "            for idx in sorted_indices\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeepSeek-R1-Distill-Llama-70B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\")\n",
    "\n",
    "model_path = \"models/models--deepseek-ai--DeepSeek-R1-Distill-Llama-70B/snapshots/0d6d11a6ea1187363aa7b78543f824fc02e06b14\"\n",
    "\n",
    "deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-llama-70b_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-llama-70b.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n",
    "system_prompt = \"\"\n",
    "df_solutions = pd.DataFrame()\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0001,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"do_sample\": True, # False is equal to temperature = 0\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    task = algorithms[\"content\"][i]\n",
    "    task_id = algorithms[\"id\"][i]\n",
    "    task_name = algorithms[\"title\"][i]\n",
    "    language = algorithms[\"language\"][i]\n",
    "    prompt = f\"Solve this programming task, you need to return only program code in {language}, \\\n",
    "    don't return anything else. Task: {task}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = deepseek_r1_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    text += \"<think>\\n\\n</think>\\n\\n\"\n",
    "    \n",
    "    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n",
    "    \n",
    "    generated_ids = deepseek_r1_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        max_new_tokens=generation_config[\"max_new_tokens\"],\n",
    "        do_sample=generation_config[\"do_sample\"],\n",
    "        # finish_reason=\"stop\",\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    df_solutions.loc[i, \"task_id\"] = task_id\n",
    "    df_solutions.loc[i, \"task_name\"] = task_name\n",
    "    df_solutions.loc[i, \"task\"] = task\n",
    "    df_solutions.loc[i, \"language\"] = language\n",
    "    df_solutions.loc[i, \"prompt\"] = prompt\n",
    "    df_solutions.loc[i, \"solution\"] = response[0]\n",
    "    try:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = response[0].strip().split(\"```\")[1]\n",
    "    except:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = \"No code\"\n",
    "    print(i)\n",
    "\n",
    "df_solutions.to_csv(\"deepseek-r1-llama-70b-algorithms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeepSeek-R1-Distill-Llama-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\")\n",
    "model_path = \"models/models--deepseek-ai--DeepSeek-R1-Distill-Llama-8B/snapshots/74fbf131a939963dd1e244389bb61ad0d0440a4d\"\n",
    "device = \"cuda\"\n",
    "\n",
    "deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "deepseek_r1_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "deepseek_r1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-llama-4b_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-llama-4b.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n",
    "system_prompt = \"\"\n",
    "df_solutions = pd.DataFrame()\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0001,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"do_sample\": True, # False is equal to temperature = 0\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    task = algorithms[\"content\"][i]\n",
    "    task_id = algorithms[\"id\"][i]\n",
    "    task_name = algorithms[\"title\"][i]\n",
    "    language = algorithms[\"language\"][i]\n",
    "    prompt = f\"Solve this programming task, you need to return only program code in {language}, \\\n",
    "    don't return anything else. Task: {task}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = deepseek_r1_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    text += \"<think>\\n\\n</think>\\n\\n\"\n",
    "    \n",
    "    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n",
    "    \n",
    "    generated_ids = deepseek_r1_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        max_new_tokens=generation_config[\"max_new_tokens\"],\n",
    "        do_sample=generation_config[\"do_sample\"],\n",
    "        # finish_reason=\"stop\",\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    df_solutions.loc[i, \"task_id\"] = task_id\n",
    "    df_solutions.loc[i, \"task_name\"] = task_name\n",
    "    df_solutions.loc[i, \"task\"] = task\n",
    "    df_solutions.loc[i, \"language\"] = language\n",
    "    df_solutions.loc[i, \"prompt\"] = prompt\n",
    "    df_solutions.loc[i, \"solution\"] = response[0]\n",
    "    try:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = response[0].strip().split(\"```\")[1]\n",
    "    except:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = \"No code\"\n",
    "    print(i)\n",
    "\n",
    "df_solutions.to_csv(\"deepseek-r1-llama-8b-algorithms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeepSeek-R1-Distill-Qwen-32B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13453f0ba23d42bf95f8253036454450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")\n",
    "\n",
    "model_path = \"models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-32B/snapshots/3865e12a1eb7cbd641ab3f9dfc28c588c6b0c1e9\"\n",
    "device = \"cuda\"\n",
    "\n",
    "deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "with open(\"deepseek-r1-qwen-32b_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-32b.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n",
    "system_prompt = \"\"\n",
    "df_solutions = pd.DataFrame()\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0001,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"do_sample\": True, # False is equal to temperature = 0\n",
    "}\n",
    "\n",
    "for i in range(15):\n",
    "    task = algorithms[\"content\"][i]\n",
    "    task_id = algorithms[\"id\"][i]\n",
    "    task_name = algorithms[\"title\"][i]\n",
    "    language = algorithms[\"language\"][i]\n",
    "    prompt = f\"Solve this programming task, you need to return only program code in {language}, \\\n",
    "    don't return anything else. Task: {task}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = deepseek_r1_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    text += \"<think>\\n\\n</think>\\n\\n\"\n",
    "    \n",
    "    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n",
    "    \n",
    "    generated_ids = deepseek_r1_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        max_new_tokens=generation_config[\"max_new_tokens\"],\n",
    "        do_sample=generation_config[\"do_sample\"],\n",
    "        # finish_reason=\"stop\",\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    df_solutions.loc[i, \"task_id\"] = task_id\n",
    "    df_solutions.loc[i, \"task_name\"] = task_name\n",
    "    df_solutions.loc[i, \"task\"] = task\n",
    "    df_solutions.loc[i, \"language\"] = language\n",
    "    df_solutions.loc[i, \"prompt\"] = prompt\n",
    "    df_solutions.loc[i, \"solution\"] = response[0]\n",
    "    try:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = response[0].strip().split(\"```\")[1]\n",
    "    except:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = \"No code\"\n",
    "    print(i)\n",
    "\n",
    "df_solutions.to_csv(\"deepseek-r1-qwen-32b-algorithms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeepSeek-R1-Distill-Qwen-14B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab9be9708374f3c927d75270ca2308f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\")\n",
    "\n",
    "model_path = \"models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-14B/snapshots/5ee96d8a09692e87087a6e0496d87124a1cdc3fe\"\n",
    "device = \"cuda\"\n",
    "\n",
    "deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-14b_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-14b.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n",
    "system_prompt = \"\"\n",
    "df_solutions = pd.DataFrame()\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0001,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"do_sample\": True, # False is equal to temperature = 0\n",
    "}\n",
    "\n",
    "for i in range(15):\n",
    "    task = algorithms[\"content\"][i]\n",
    "    task_id = algorithms[\"id\"][i]\n",
    "    task_name = algorithms[\"title\"][i]\n",
    "    language = algorithms[\"language\"][i]\n",
    "    prompt = f\"Solve this programming task, you need to return only program code in {language}, \\\n",
    "    don't return anything else. Task: {task}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = deepseek_r1_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    text += \"<think>\\n\\n</think>\\n\\n\"\n",
    "    \n",
    "    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n",
    "    \n",
    "    generated_ids = deepseek_r1_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        max_new_tokens=generation_config[\"max_new_tokens\"],\n",
    "        do_sample=generation_config[\"do_sample\"],\n",
    "        # finish_reason=\"stop\",\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    df_solutions.loc[i, \"task_id\"] = task_id\n",
    "    df_solutions.loc[i, \"task_name\"] = task_name\n",
    "    df_solutions.loc[i, \"task\"] = task\n",
    "    df_solutions.loc[i, \"language\"] = language\n",
    "    df_solutions.loc[i, \"prompt\"] = prompt\n",
    "    df_solutions.loc[i, \"solution\"] = response[0]\n",
    "    try:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = response[0].strip().split(\"```\")[1]\n",
    "    except:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = \"No code\"\n",
    "    print(i)\n",
    "\n",
    "df_solutions.to_csv(\"deepseek-r1-qwen-14b-algorithms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeepSeek-R1-Distill-Qwen-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\")\n",
    "\n",
    "model_path = \"models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/6602cadec947dbb53e64f3d8d6425320b2197247\"\n",
    "device = \"cuda\"\n",
    "\n",
    "deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "deepseek_r1_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "deepseek_r1_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-7B_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-7B.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n",
    "system_prompt = \"\"\n",
    "df_solutions = pd.DataFrame()\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0001,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"do_sample\": True, # False is equal to temperature = 0\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    task = algorithms[\"content\"][i]\n",
    "    task_id = algorithms[\"id\"][i]\n",
    "    task_name = algorithms[\"title\"][i]\n",
    "    language = algorithms[\"language\"][i]\n",
    "    prompt = f\"Solve this programming task, you need to return only program code in {language}, \\\n",
    "    don't return anything else. Task: {task}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = deepseek_r1_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    text += \"<think>\\n\\n</think>\\n\\n\"\n",
    "    \n",
    "    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n",
    "    \n",
    "    generated_ids = deepseek_r1_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        max_new_tokens=generation_config[\"max_new_tokens\"],\n",
    "        do_sample=generation_config[\"do_sample\"],\n",
    "        # finish_reason=\"stop\",\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    df_solutions.loc[i, \"task_id\"] = task_id\n",
    "    df_solutions.loc[i, \"task_name\"] = task_name\n",
    "    df_solutions.loc[i, \"task\"] = task\n",
    "    df_solutions.loc[i, \"language\"] = language\n",
    "    df_solutions.loc[i, \"prompt\"] = prompt\n",
    "    df_solutions.loc[i, \"solution\"] = response[0]\n",
    "    try:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = response[0].strip().split(\"```\")[1]\n",
    "    except:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = \"No code\"\n",
    "    print(i)\n",
    "\n",
    "df_solutions.to_csv(\"deepseek-r1-qwen-7B-algorithms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DeepSeek-R1-Distill-Qwen-1.5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "# deepseek_r1_model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "model_path = \"models/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\"\n",
    "device = \"cuda\"\n",
    "\n",
    "deepseek_r1_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "deepseek_r1_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "deepseek_r1_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-1500M_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T12:00:06.406898Z",
     "iopub.status.busy": "2025-02-03T12:00:06.406593Z",
     "iopub.status.idle": "2025-02-03T12:00:06.669114Z",
     "shell.execute_reply": "2025-02-03T12:00:06.668482Z",
     "shell.execute_reply.started": "2025-02-03T12:00:06.406875Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-1500M-missed-tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    warm_start_result = warm_start_gpu(misswords, deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "    results[f\"misswords\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-1500M.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "            # results[f\"language_{i}\"][f\"{i}_{j}_{keywords[j]}\"] = warm_start_result\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-04T12:26:53.461213Z",
     "iopub.status.busy": "2025-02-04T12:26:53.460887Z",
     "iopub.status.idle": "2025-02-04T12:26:54.646930Z",
     "shell.execute_reply": "2025-02-04T12:26:54.646216Z",
     "shell.execute_reply.started": "2025-02-04T12:26:53.461189Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"deepseek-r1-qwen-1500M-missed-tokens.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    results[f\"misswords\"] = {}\n",
    "    \n",
    "    for i in range(len(misswords)):\n",
    "        warm_start_result = warm_start(misswords[i], deepseek_r1_model, deepseek_r1_tokenizer)\n",
    "        results[f\"misswords\"][i] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"You are AI-assistant which helps user to solve programming tasks.\"\n",
    "system_prompt = \"\"\n",
    "df_solutions = pd.DataFrame()\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0001,\n",
    "    # \"top_p\": 0.95,\n",
    "    \"max_new_tokens\": 4096,\n",
    "    \"do_sample\": True, # False is equal to temperature = 0\n",
    "}\n",
    "\n",
    "for i in range(100):\n",
    "    task = algorithms[\"content\"][i]\n",
    "    task_id = algorithms[\"id\"][i]\n",
    "    task_name = algorithms[\"title\"][i]\n",
    "    language = algorithms[\"language\"][i]\n",
    "    prompt = f\"Solve this programming task, you need to return only program code in {language}, \\\n",
    "    don't return anything else. Task: {task}\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = deepseek_r1_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "    text += \"<think>\\n\\n</think>\\n\\n\"\n",
    "    \n",
    "    model_inputs = deepseek_r1_tokenizer([text], return_tensors=\"pt\").to(deepseek_r1_model.device)\n",
    "    \n",
    "    generated_ids = deepseek_r1_model.generate(\n",
    "        **model_inputs,\n",
    "        temperature=generation_config[\"temperature\"],\n",
    "        max_new_tokens=generation_config[\"max_new_tokens\"],\n",
    "        do_sample=generation_config[\"do_sample\"],\n",
    "        # finish_reason=\"stop\",\n",
    "    )\n",
    "\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = deepseek_r1_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "    df_solutions.loc[i, \"task_id\"] = task_id\n",
    "    df_solutions.loc[i, \"task_name\"] = task_name\n",
    "    df_solutions.loc[i, \"task\"] = task\n",
    "    df_solutions.loc[i, \"language\"] = language\n",
    "    df_solutions.loc[i, \"prompt\"] = prompt\n",
    "    df_solutions.loc[i, \"solution\"] = response[0]\n",
    "    try:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = response[0].strip().split(\"```\")[1]\n",
    "    except:\n",
    "        df_solutions.loc[i, \"programm_code_only\"] = \"No code\"\n",
    "    print(i)\n",
    "\n",
    "df_solutions.to_csv(\"deepseek-r1-qwen-1500M-algorithms.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T10:52:01.443683Z",
     "iopub.status.busy": "2025-02-05T10:52:01.443302Z",
     "iopub.status.idle": "2025-02-05T10:52:01.447137Z",
     "shell.execute_reply": "2025-02-05T10:52:01.446299Z",
     "shell.execute_reply.started": "2025-02-05T10:52:01.443653Z"
    }
   },
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T11:16:25.459691Z",
     "iopub.status.busy": "2025-02-05T11:16:25.459292Z",
     "iopub.status.idle": "2025-02-05T11:16:25.463474Z",
     "shell.execute_reply": "2025-02-05T11:16:25.462422Z",
     "shell.execute_reply.started": "2025-02-05T11:16:25.459666Z"
    }
   },
   "outputs": [],
   "source": [
    "# full_response = response[0].strip().split(\"```\")\n",
    "\n",
    "# print(len(full_response))\n",
    "# print(full_response[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-05T11:16:29.938920Z",
     "iopub.status.busy": "2025-02-05T11:16:29.938550Z",
     "iopub.status.idle": "2025-02-05T11:16:29.954866Z",
     "shell.execute_reply": "2025-02-05T11:16:29.953822Z",
     "shell.execute_reply.started": "2025-02-05T11:16:29.938892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>task_name</th>\n",
       "      <th>task</th>\n",
       "      <th>language</th>\n",
       "      <th>prompt</th>\n",
       "      <th>solution</th>\n",
       "      <th>programm_code_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1568.0</td>\n",
       "      <td>Minimum Number of Days to Disconnect Island</td>\n",
       "      <td>You are given an `m x n` binary grid `grid` wh...</td>\n",
       "      <td>python</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>python\\ndef min_days_to_disconnect(grid):\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1714.0</td>\n",
       "      <td>Sum Of Special Evenly-Spaced Elements In Array</td>\n",
       "      <td>You are given a **0-indexed** integer array `n...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to efficiently ...</td>\n",
       "      <td>javascript\\nconst MOD = 10**9 + 7;\\n\\nfunction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1326.0</td>\n",
       "      <td>Minimum Number of Taps to Open to Water a Garden</td>\n",
       "      <td>There is a one-dimensional garden on the x-axi...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nfunction minTaps(n, ranges) {\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>410.0</td>\n",
       "      <td>Split Array Largest Sum</td>\n",
       "      <td>Given an integer array `nums` and an integer `...</td>\n",
       "      <td>python</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to split an int...</td>\n",
       "      <td>python\\ndef minimize_largest_sum(nums, k):\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>87.0</td>\n",
       "      <td>Scramble String</td>\n",
       "      <td>We can scramble a string s to get a string t u...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine if...</td>\n",
       "      <td>javascript\\nfunction isScrambled(s1, s2) {\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>996.0</td>\n",
       "      <td>Number of Squareful Arrays</td>\n",
       "      <td>An array is **squareful** if the sum of every ...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nfunction countSquarefulPermutation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1231.0</td>\n",
       "      <td>Divide Chocolate</td>\n",
       "      <td>You have one chocolate bar that consists of so...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nfunction maximizeMinimumSweetness(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1815.0</td>\n",
       "      <td>Maximum Number of Groups Getting Fresh Donuts</td>\n",
       "      <td>There is a donuts shop that bakes donuts in ba...</td>\n",
       "      <td>python</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>python\\ndef max_happy_groups(batchSize, groups...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1340.0</td>\n",
       "      <td>Jump Game V</td>\n",
       "      <td>Given an array of integers `arr` and an intege...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nfunction maxIndices(arr, d) {\\n   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1210.0</td>\n",
       "      <td>Minimum Moves to Reach Target with Rotations</td>\n",
       "      <td>In an `n*n` grid, there is a snake that spans ...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nfunction solve(grid) {\\n    const ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1776.0</td>\n",
       "      <td>Car Fleet II</td>\n",
       "      <td>There are `n` cars traveling at different spee...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nfunction carsCollisionTime(cars) {...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>699.0</td>\n",
       "      <td>Falling Squares</td>\n",
       "      <td>There are several squares being dropped onto t...</td>\n",
       "      <td>python</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>python\\ndef positions_to_heights(positions):\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>879.0</td>\n",
       "      <td>Profitable Schemes</td>\n",
       "      <td>There is a group of `n` members, and a list of...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "      <td>javascript\\nconst MOD = 10**9 + 7;\\n\\nfunction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>726.0</td>\n",
       "      <td>Number of Atoms</td>\n",
       "      <td>Given a string `formula` representing a chemic...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to parse a chem...</td>\n",
       "      <td>javascript\\nfunction parseFormula(formula) {\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1235.0</td>\n",
       "      <td>Maximum Profit in Job Scheduling</td>\n",
       "      <td>We have `n` jobs, where every job is scheduled...</td>\n",
       "      <td>javascript</td>\n",
       "      <td>Solve this programming task, you need to retur...</td>\n",
       "      <td>To solve this problem, we need to select a sub...</td>\n",
       "      <td>javascript\\nfunction maximizeProfit(startTime,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_id                                         task_name  \\\n",
       "0    1568.0       Minimum Number of Days to Disconnect Island   \n",
       "1    1714.0    Sum Of Special Evenly-Spaced Elements In Array   \n",
       "2    1326.0  Minimum Number of Taps to Open to Water a Garden   \n",
       "3     410.0                           Split Array Largest Sum   \n",
       "4      87.0                                   Scramble String   \n",
       "5     996.0                        Number of Squareful Arrays   \n",
       "6    1231.0                                  Divide Chocolate   \n",
       "7    1815.0     Maximum Number of Groups Getting Fresh Donuts   \n",
       "8    1340.0                                       Jump Game V   \n",
       "9    1210.0      Minimum Moves to Reach Target with Rotations   \n",
       "10   1776.0                                      Car Fleet II   \n",
       "11    699.0                                   Falling Squares   \n",
       "12    879.0                                Profitable Schemes   \n",
       "13    726.0                                   Number of Atoms   \n",
       "14   1235.0                  Maximum Profit in Job Scheduling   \n",
       "\n",
       "                                                 task    language  \\\n",
       "0   You are given an `m x n` binary grid `grid` wh...      python   \n",
       "1   You are given a **0-indexed** integer array `n...  javascript   \n",
       "2   There is a one-dimensional garden on the x-axi...  javascript   \n",
       "3   Given an integer array `nums` and an integer `...      python   \n",
       "4   We can scramble a string s to get a string t u...  javascript   \n",
       "5   An array is **squareful** if the sum of every ...  javascript   \n",
       "6   You have one chocolate bar that consists of so...  javascript   \n",
       "7   There is a donuts shop that bakes donuts in ba...      python   \n",
       "8   Given an array of integers `arr` and an intege...  javascript   \n",
       "9   In an `n*n` grid, there is a snake that spans ...  javascript   \n",
       "10  There are `n` cars traveling at different spee...  javascript   \n",
       "11  There are several squares being dropped onto t...      python   \n",
       "12  There is a group of `n` members, and a list of...  javascript   \n",
       "13  Given a string `formula` representing a chemic...  javascript   \n",
       "14  We have `n` jobs, where every job is scheduled...  javascript   \n",
       "\n",
       "                                               prompt  \\\n",
       "0   Solve this programming task, you need to retur...   \n",
       "1   Solve this programming task, you need to retur...   \n",
       "2   Solve this programming task, you need to retur...   \n",
       "3   Solve this programming task, you need to retur...   \n",
       "4   Solve this programming task, you need to retur...   \n",
       "5   Solve this programming task, you need to retur...   \n",
       "6   Solve this programming task, you need to retur...   \n",
       "7   Solve this programming task, you need to retur...   \n",
       "8   Solve this programming task, you need to retur...   \n",
       "9   Solve this programming task, you need to retur...   \n",
       "10  Solve this programming task, you need to retur...   \n",
       "11  Solve this programming task, you need to retur...   \n",
       "12  Solve this programming task, you need to retur...   \n",
       "13  Solve this programming task, you need to retur...   \n",
       "14  Solve this programming task, you need to retur...   \n",
       "\n",
       "                                             solution  \\\n",
       "0   To solve this problem, we need to determine th...   \n",
       "1   To solve this problem, we need to efficiently ...   \n",
       "2   To solve this problem, we need to determine th...   \n",
       "3   To solve this problem, we need to split an int...   \n",
       "4   To solve this problem, we need to determine if...   \n",
       "5   To solve this problem, we need to determine th...   \n",
       "6   To solve this problem, we need to determine th...   \n",
       "7   To solve this problem, we need to determine th...   \n",
       "8   To solve this problem, we need to determine th...   \n",
       "9   To solve this problem, we need to determine th...   \n",
       "10  To solve this problem, we need to determine th...   \n",
       "11  To solve this problem, we need to determine th...   \n",
       "12  To solve this problem, we need to determine th...   \n",
       "13  To solve this problem, we need to parse a chem...   \n",
       "14  To solve this problem, we need to select a sub...   \n",
       "\n",
       "                                   programm_code_only  \n",
       "0   python\\ndef min_days_to_disconnect(grid):\\n   ...  \n",
       "1   javascript\\nconst MOD = 10**9 + 7;\\n\\nfunction...  \n",
       "2   javascript\\nfunction minTaps(n, ranges) {\\n   ...  \n",
       "3   python\\ndef minimize_largest_sum(nums, k):\\n  ...  \n",
       "4   javascript\\nfunction isScrambled(s1, s2) {\\n  ...  \n",
       "5   javascript\\nfunction countSquarefulPermutation...  \n",
       "6   javascript\\nfunction maximizeMinimumSweetness(...  \n",
       "7   python\\ndef max_happy_groups(batchSize, groups...  \n",
       "8   javascript\\nfunction maxIndices(arr, d) {\\n   ...  \n",
       "9   javascript\\nfunction solve(grid) {\\n    const ...  \n",
       "10  javascript\\nfunction carsCollisionTime(cars) {...  \n",
       "11  python\\ndef positions_to_heights(positions):\\n...  \n",
       "12  javascript\\nconst MOD = 10**9 + 7;\\n\\nfunction...  \n",
       "13  javascript\\nfunction parseFormula(formula) {\\n...  \n",
       "14  javascript\\nfunction maximizeProfit(startTime,...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Qwen 2.5 Coder 32B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_path = \"models\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     cache_dir=cache_path\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "#     cache_dir=cache_path,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f362bcd3c2ac4696b20b718a9d990f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"models/models--Qwen--Qwen2.5-Coder-32B-Instruct/snapshots/381fc969f78efac66bc87ff7ddeadb7e73c218a7\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "with open(\"qwen-coder-32b_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, model, tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qwen-coder-32b.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], model, tokenizer)\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Qwen 2.5 32B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = \"models\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    cache_dir=cache_path\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",\n",
    "    cache_dir=cache_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0b5f85ea8f4c82ad69d331c3e6b822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"models/models--Qwen--Qwen2.5-32B-Instruct/snapshots/5ede1c97bbab6ce5cda5812749b4c0bdf79b18dd\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "with open(\"qwen-32b_sequence.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "    \n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        warm_start_result = warm_start_sequence(keywords, model, tokenizer)\n",
    "        results[f\"language_{i}\"] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qwen-32b.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    results = {}\n",
    "\n",
    "    for i in range(len(all_keywords)):\n",
    "        keywords = all_keywords[i]\n",
    "        results[f\"language_{i}\"] = {}\n",
    "        for j in range(len(keywords)):\n",
    "            warm_start_result = warm_start_token(keywords[j], model, tokenizer)\n",
    "            results[f\"language_{i}\"][j] = warm_start_result\n",
    "\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miss words in tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T11:31:14.132346Z",
     "iopub.status.busy": "2025-01-22T11:31:14.131900Z",
     "iopub.status.idle": "2025-01-22T11:31:14.136957Z",
     "shell.execute_reply": "2025-01-22T11:31:14.135923Z",
     "shell.execute_reply.started": "2025-01-22T11:31:14.132309Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T11:46:37.883912Z",
     "iopub.status.busy": "2025-01-22T11:46:37.883595Z",
     "iopub.status.idle": "2025-01-22T11:46:37.888916Z",
     "shell.execute_reply": "2025-01-22T11:46:37.887912Z",
     "shell.execute_reply.started": "2025-01-22T11:46:37.883889Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from termcolor import colored\n",
    "\n",
    "colors = ['red', 'green', 'yellow', 'blue']\n",
    "\n",
    "def tokenize_and_colorize(tokenizer, code):\n",
    "    tokens = tokenizer.tokenize(code)\n",
    "    colored_code = []\n",
    "    last_color = None\n",
    "\n",
    "    for tok in tokens:\n",
    "        new_color = random.choice([color for color in colors if color != last_color])\n",
    "        last_color = new_color\n",
    "        colored_code.append(colored(tok, new_color))\n",
    "\n",
    "    return ' '.join(colored_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepseek r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T11:46:40.549892Z",
     "iopub.status.busy": "2025-01-22T11:46:40.549551Z",
     "iopub.status.idle": "2025-01-22T11:46:40.564981Z",
     "shell.execute_reply": "2025-01-22T11:46:40.564079Z",
     "shell.execute_reply.started": "2025-01-22T11:46:40.549860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non local\n",
      "s ynchronized\n",
      "trans ient\n",
      "strict fp\n",
      "fall through\n",
      "complex 6 4\n",
      "complex 1 2 8\n",
      "float 3 2\n",
      "use Effect\n",
      "use Context\n",
      "use Reducer\n",
      "use Memo\n",
      "use Callback\n",
      "use Ref\n",
      "use Layout Effect\n",
      "use Im per ative Handle\n",
      "virtual DOM\n",
      "re conciliation\n",
      "s usp ense\n",
      "error Boundary\n",
      "use Strict Mode\n",
      "instance of\n",
      "end declare\n",
      "end for\n",
      "ends witch\n",
      "end while\n"
     ]
    }
   ],
   "source": [
    "for missword in misswords:\n",
    "    colored_code = tokenize_and_colorize(deepseek_tokenizer, missword)\n",
    "    print(colored_code)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6525356,
     "sourceId": 10546507,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6579029,
     "sourceId": 10625817,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6589820,
     "sourceId": 10642913,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
